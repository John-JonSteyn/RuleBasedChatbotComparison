#separator:tab
#html:false
#guid column:1
#notetype column:2
#deck column:3
n?XsAzq|Os	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is data science?	Data science is an interdisciplinary field that combines statistics, programming, and domain knowledge to extract meaningful insights from data so organizations can make evidence-based decisions.
pa2,KP73=^	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does data science differ from analytics and artificial intelligence?	Data analytics focuses on describing and diagnosing what happened, artificial intelligence focuses on making machines act intelligently, and data science spans the pipeline—collecting, cleaning, modeling, and deploying insights that can power analytics and AI systems.
b[UR`oST+$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is domain expertise important in data science?	Domain expertise is crucial because it frames the right questions, interprets results in context, and prevents technically correct models from producing irrelevant or misleading recommendations.
p}hgZ1!]4Y	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the core components of a data science workflow?	The core components typically include data collection, data cleaning, exploratory data analysis (EDA), feature engineering, modeling, evaluation, and deployment with ongoing monitoring.
m$*%`0j<m{	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the purpose of data cleaning?	Data cleaning removes errors, inconsistencies, duplicates, and missing or out-of-range values so that analyses and models are built on accurate, reliable inputs.
N;KCRX^B<Z	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is exploratory data analysis (EDA)?	Exploratory data analysis is the process of visualizing and summarizing data to understand distributions, spot anomalies, generate hypotheses, and choose appropriate modeling approaches.
dW-&l5!LH,	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is data processing different from data cleaning?	Data cleaning fixes quality issues, while data processing transforms data into usable forms—such as aggregations, encodings, joins, and feature creation—for analysis or modeling.
f@GNDPnHId	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the difference between structured and unstructured data?	Structured data fits into predefined schemas like tables, while unstructured data lacks a fixed format and includes text, images, audio, and video that require specialized preprocessing.
ew/D5B[0qD	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a relational (SQL) database best suited for?	Relational databases are best for strongly structured data with well-defined relationships, transactional integrity, and complex queries that benefit from ACID guarantees.
ha^>40>Q*S	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When is a NoSQL database a better choice?	NoSQL databases are well-suited for high-volume, rapidly changing, or schema-flexible data and horizontal scalability needs common in big data applications.
IPvAo*Ob_G	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is big data?	Big data refers to datasets whose size, speed, and diversity exceed the capabilities of traditional tools, requiring specialized storage, processing, and analytics techniques.
y:%&7V:4,H	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the “five Vs” of big data?	The five Vs are volume (amount of data), velocity (speed of generation), variety (different forms), veracity (data quality), and value (usefulness for decisions).
uv8FCgy~T3	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do organizations turn big data into competitive advantage?	Organizations gain advantage by combining high-quality data with timely analytics to personalize experiences, optimize operations, detect anomalies, and inform strategic decisions.
MEX%%h%Di/	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is artificial intelligence (AI)?	Artificial intelligence is the field of making computers perform tasks that typically require human intelligence, such as perception, reasoning, and decision-making.
g8qe`A?5Q*	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is machine learning (ML)?	Machine learning is a subset of AI where systems learn patterns from data and improve performance on a task without being explicitly programmed with rule-based logic.
H52S84JFS(	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the difference between weak AI and strong AI?	Weak (narrow) AI is specialized for a specific task like voice assistance, while strong (general) AI is a theoretical form that could perform any intellectual task a human can.
OCPrAukAWN	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do classification and regression differ in supervised learning?	Classification predicts discrete categories (e.g., fraud or not), while regression predicts continuous values (e.g., expected sales next week).
f07:ri2]Zx	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why does overfitting harm machine learning models?	Overfitting harms models because they memorize noise in the training data and fail to generalize to new, unseen data, reducing real-world accuracy.
I(MFc.Fg%x	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do train/validation/test splits help model quality?	Train/validation/test splits help by fitting on one set, tuning on a second, and objectively evaluating on a held-out third set to estimate true performance.
D<:yt%f}U/	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is feature engineering and why does it matter?	Feature engineering creates informative variables from raw data, often improving model accuracy more than algorithm changes by exposing signal the model can learn.
AR%ws8Fn$I	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is AI used in healthcare?	AI supports healthcare by assisting diagnosis from images, triaging cases, predicting risks, and personalizing treatments to improve outcomes and efficiency.
Lp{t21h4lJ	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is AI used in finance?	AI in finance supports fraud detection, credit risk scoring, algorithmic trading, and customer service chatbots that operate continuously and consistently.
Q5ZaV!5>~M	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is AI used in customer service?	AI improves customer service with chatbots and virtual assistants that handle repetitive inquiries, escalate complex issues, and provide consistent responses at scale.
IQVp`c1JIM	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is robotic process automation (RPA)?	Robotic process automation uses software bots to perform repetitive, rules-based tasks like data entry and report generation, improving speed and reducing errors.
m[nXs$~7:4	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does AI enhance decision-making from large datasets?	AI enhances decisions by detecting patterns, anomalies, and trends at scales humans cannot process, providing timely insights for actions such as fraud alerts or resource allocation.
sN~$b&Y&8/	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is AI applied in robotics and autonomous systems?	AI enables robots and self-driving vehicles to perceive environments, plan motions, and act safely by fusing sensor data and making real-time decisions.
E,K3}tAq?C	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What ethical risks accompany AI deployment?	Ethical risks include algorithmic bias, privacy violations, opaque decisions, security vulnerabilities, and unclear accountability when automated systems make mistakes.
iBLSiWr|VG	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can teams mitigate algorithmic bias?	Teams mitigate bias by using diverse, representative data, auditing models with fairness metrics, removing or debiasing sensitive features, and monitoring outcomes post-deployment.
yU{HtNEiva	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is privacy central to AI and data science?	Privacy is central because models often rely on personal data; respecting consent, minimizing data collection, and protecting data with security controls builds trust and meets legal obligations.
u&}H`P1)Qm	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the role of transparency and explainability in AI?	Transparency and explainability help stakeholders understand why a model made a decision, enabling accountability, debugging, regulatory compliance, and user trust.
k~!7t>ivwh	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is explainable AI (XAI)?	Explainable AI refers to methods and tools that make model behavior interpretable—such as feature attributions, example-based explanations, and inherently transparent models.
v:>m9XipNi	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is edge AI and why use it?	Edge AI runs models on local devices to reduce latency, preserve privacy, and maintain functionality when connectivity is limited, enabling real-time decisions.
"eihm(#YXjM"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is quantum AI?	Quantum AI explores using quantum computing to accelerate certain AI tasks—like optimization or sampling—that are hard for classical computers.
LPzwd~s9/4	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is AutoML?	Automated machine learning (AutoML) automates steps like feature selection, model selection, and hyperparameter tuning so practitioners can build models faster with less manual effort.
AIFylmRtfa	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can AI, IoT, and blockchain work together?	AI can analyze IoT data for intelligence, IoT devices provide rich real-time inputs, and blockchain can add tamper-resistant audit trails for device events and automated transactions.
O~@-H/{tQ@	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a smart city in the context of AI and data science?	A smart city uses sensors, connectivity, and analytics to optimize services like transportation, energy, water, waste, and public safety while improving quality of life.
bE2qD:Av%$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does AI improve traffic management?	AI improves traffic by adapting signal timing in real time, predicting congestion, optimizing routing, and supporting autonomous and connected vehicles to reduce delays and accidents.
E|`H6tqtgI	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does AI support waste and utility management?	AI supports waste and utilities by using sensor data to schedule pickups when bins are full, detect leaks, forecast demand, and balance energy loads via smart grids.
s[~h:_PEiY	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What concerns arise with AI for public safety?	AI for public safety raises concerns about surveillance overreach, false positives, demographic bias, and due process, requiring strong governance and safeguards.
PsRc{dF8sb	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does Netflix personalize content using AI?	Netflix personalizes content by analyzing viewing history and behavior to recommend titles likely to engage each user, increasing satisfaction and retention.
zw^h;4_Hsn	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is collaborative filtering in recommendations?	Collaborative filtering recommends items by finding users with similar tastes and suggesting what those similar users enjoyed, leveraging collective behavior patterns.
b=4Fz1r$``	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is content-based filtering in recommendations?	Content-based filtering recommends items with similar attributes—like genre, cast, or themes—to what a user has liked before, focusing on item characteristics.
E,yR7mp/N^	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How else does AI optimize streaming platforms beyond recommendations?	AI also optimizes video quality by adapting to network conditions, generates thumbnails tailored to viewer preferences, and informs content planning through audience insights.
peUW&cKZ4c	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What privacy concerns accompany personalization systems?	Personalization can raise concerns because it collects and infers sensitive preferences; clear consent, data minimization, security, and opt-out controls help address them.
efAKIth}9.	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are common data privacy and cybersecurity threats today?	Common threats include data breaches, phishing, ransomware, API abuse, insider misuse, and model inversion or poisoning attacks against AI systems.
c7rr.]9tER	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can organizations reduce breach risk?	Organizations reduce risk by encrypting data, enforcing multi-factor authentication, segmenting networks, patching promptly, training users, and regularly auditing access and configurations.
oYp3(*|h)V	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is AI used by attackers and defenders in cybersecurity?	Attackers use AI to automate phishing and evade detection, while defenders use AI to detect anomalies, triage alerts, and respond faster to emerging threats.
Iv<B5e@xLZ	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why are data ownership and consent important?	Data ownership and consent matter because individuals should control how their information is collected, shared, and used, and organizations must be transparent and honor user choices.
GWJ^k1<UwE	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What role does cloud computing play in AI and data science?	Cloud computing provides elastic compute, storage, and managed ML services so teams can train, deploy, and scale models without maintaining on-premises infrastructure.
KB9YuE,5yY	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does the cloud accelerate machine learning projects?	The cloud accelerates ML by offering on-demand GPUs/TPUs, managed data pipelines, and ready-made frameworks and APIs that shorten setup time and scale experiments.
IGh`g0Vl/Q	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does the cloud enable collaboration on data projects?	The cloud enables collaboration by allowing shared datasets, notebooks, and pipelines accessible globally, which speeds research, education, and cross-team development.
tX0js^8S_o	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What challenges come with cloud-based AI?	Cloud-based AI must address data security, access control, cost governance, and data sovereignty requirements that dictate where data can be stored and processed.
RdYY*0)al*	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is responsible AI governance?	Responsible AI governance is a set of processes and roles that ensure models are ethical, compliant, secure, monitored for drift and bias, and aligned with business and societal values.
rInNc4Vxq[	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can teams evaluate fairness in models?	Teams evaluate fairness by selecting relevant fairness metrics (e.g., demographic parity, equal opportunity), comparing across groups, and remediating disparities while monitoring over time.
I+AlQk&).$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are sensible first steps for an ML project?	Sensible steps include defining the problem and success metrics, securing data and consent, building a baseline, iterating with validation, and planning deployment and monitoring.
tm4z$f2%.R	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do business intelligence, analytics, and data science relate?	Business intelligence reports what happened, analytics explains why and sometimes forecasts, and data science builds end-to-end solutions that may include predictive and prescriptive models.
i^3&;>R5Sq	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why should technologists study data ethics?	Technologists should study data ethics to anticipate harms, design safeguards, comply with laws, and earn user trust while delivering innovations that benefit society.
J/*yAC&J$E	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Which skills will matter most as AI and data science evolve?	Skills that matter include statistical thinking, software engineering, MLOps, UX for explainability, security and privacy literacy, and cross-disciplinary communication.
wS5~5@K!^E	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is predictive healthcare and why does it matter?	Predictive healthcare uses machine learning to analyze data like EHRs, images, labs, sensors, and genomics to forecast risks, detect disease early, recommend treatments, and optimize hospital operations so clinicians can intervene sooner and improve outcomes.
Mr)[nX&P0`	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Which data sources are most common in predictive healthcare?	Common sources include electronic health records, medical images (X-ray, CT, MRI), pathology slides, vitals and waveforms, wearables and remote sensors, medication and procedure histories, claims, genomics/proteomics, and social determinants of health.
fKcdA@lcOM	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do ML models improve medical imaging diagnostics?	Models—often convolutional or transformer-based—learn patterns from labeled scans to flag abnormalities (e.g., nodules, bleeds, fractures), assist triage, and provide second reads that can reduce misses and speed reporting.
qgn-;NqpyC	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What evaluation metrics are appropriate for imaging AI?	Useful metrics include AUROC, AUPRC for imbalanced data, sensitivity/specificity, F1, localization overlap (IoU), reader-study gains, calibration curves, and time-to-diagnosis improvements.
k<i~UBKWcc	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is AI used for early warning and deterioration detection?	By streaming vitals, labs, and nursing notes into temporal models (e.g., gradient boosting, RNNs, transformers) to predict sepsis, cardiac arrest, or readmission, enabling alerts and protocolized responses.
ngQZa{jNZE	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What role does IBM Watson-style systems play in oncology?	Decision-support systems can cross-reference literature and patient features to surface evidence-based regimens, but they must be validated locally and treated as advisory tools, not autonomous prescribers.
H3aB/Anl[/	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does telemedicine benefit from AI?	AI analyzes remote patient monitoring data (heart rate, SpO₂, BP, glucose) to detect anomalies, prioritize outreach, and personalize care plans while maintaining continuity outside clinic walls.
C~asE)U(Q7	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is AI’s role in hospital operations?	Demand forecasting, bed and ICU capacity planning, ED triage, OR block scheduling, staffing optimization, and supply chain forecasting reduce bottlenecks and improve throughput.
NfS_jkmBu@	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does AI accelerate drug discovery?	Models perform QSAR prediction, virtual screening, de-novo molecule generation, protein–ligand docking, and multi-objective optimization, shrinking candidate spaces before wet-lab validation.
f@>;;a|BNd	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What was AlphaFold’s impact on biomedicine?	Predicted protein structures at scale, enabling structure-guided target selection, pocket identification, and hypothesis generation that can speed up early-stage discovery.
A31wt66`d?	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Can AI help predict outbreaks like COVID-19?	Yes—by combining mobility, testing, wastewater, clinical, and search trends with mechanistic models, ML can project hotspots, resource needs, and policy effects, though uncertainty must be communicated.
F(Xm*v2OEf	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why must clinical AI be explainable and unbiased?	Because patient safety, trust, and regulation demand models whose reasoning and error modes can be scrutinized; opaque or biased systems risk unfair care and clinical harm.
PZeZkH]nF$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What privacy practices protect patient data in AI projects?	De-identification, access controls, encryption in transit/at rest, audit trails, data minimization, purpose limitation, differential privacy, secure enclaves, and federated learning help protect PHI.
r*Vf<G(7.N	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is federated learning in healthcare?	A method where models train across institutions without centralizing data; only gradients or weights move, reducing privacy risks while leveraging multi-site diversity.
gAranO%2`&	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is model calibration vital in medicine?	Well-calibrated probabilities let clinicians interpret risk meaningfully (e.g., a “30% sepsis risk” behaves like 30%), supporting thresholds, shared decisions, and resource planning.
s80_7w6!=1	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you monitor clinical AI after deployment?	Track data drift, performance by subgroup, alert volumes, calibration, clinician override rates, adverse events, and feedback; retrain with governance when drift or bias emerges.
"A}f=cj7#oY"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is “dataset shift” and why is it dangerous?	Dataset shift occurs when deployment data differ from training data (new devices, protocols, populations); it can silently degrade accuracy and equity unless monitored.
h+K@`>,gE[	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are common sources of bias in healthcare AI?	Representation bias (under-sampled groups), measurement bias (device or coding differences), label bias (subjective ground truth), historical bias (care access), and proxy variables that encode sensitive traits.
x!5B!.@,t?	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can teams mitigate bias in models?	Start with inclusive data and thoughtful problem framing; apply reweighting, representation learning, adversarial debiasing, group-aware thresholds, fairness constraints, and continuous subgroup monitoring.
C+Oku_lckK	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What fairness metrics are used in practice?	Demographic parity, equalized odds, equal opportunity, predictive parity, calibration within groups, and error-rate balance; the right metric depends on the clinical context and harm trade-offs.
"x!iwRPEr#H"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is explainability useful beyond compliance?	It helps debug spurious correlations, improves clinician acceptance, informs risk communication, and reveals where additional data or workflow changes would yield the biggest gains.
bV;c,~O$;q	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What tools provide model explanations?	For tabular/text: SHAP, LIME, counterfactuals, partial dependence; for images: saliency/Grad-CAM, concept activation vectors; for sequence: attention visualizations and feature attributions over time.
O+O_;Ak{Wg	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What governance structures support safe AI?	An AI governance board, model inventory, risk classification, documentation (model cards, datasheets), pre-deployment testing, human-in-the-loop policies, and post-market surveillance.
OeEgoB=G:C	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Who is accountable when AI errs in care?	Organizations should define shared accountability among developers, deploying institutions, and clinicians; clear scope-of-use, audit trails, and human-oversight checkpoints are essential.
xiU{CJ-SOC	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What regulations shape health data use?	Privacy laws (e.g., HIPAA/CCPA/GDPR equivalents), medical device rules for software (e.g., SaMD pathways), and security standards govern data handling, validation, and change control.
hr$[x7/6ly	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do “Good ML Practices” translate to clinics?	Data quality management, versioning, reproducible pipelines, robust validation, human-factors testing, real-world performance monitoring, and controlled updates with rollback plans.
qX.,f(5KmX	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the difference between assistive and autonomous AI?	Assistive systems provide recommendations with human final say; autonomous systems act without immediate human approval and require much stricter validation and safeguards.
qFuCwN@qK{	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When should clinicians override an AI?	Whenever AI output conflicts with patient context, observed signs, or values; systems should make overrides easy and record rationales to improve future model iterations.
"K{#~l$|mv<"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you prevent label leakage in EHR models?	Exclude post-outcome features, time-lock variables to prediction time, avoid proxies that only appear after diagnosis, and simulate real-time availability during training.
xTXc!$;X]u	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the value of prospective validation?	Prospective, real-time testing in the intended setting reveals workflow effects, alert fatigue, and hidden biases that retrospective AUCs cannot capture.
icGQp)v`@1	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you choose thresholds for clinical alerts?	Use decision curves, cost–benefit analyses, and stakeholder input to balance sensitivity, specificity, workload, and resource constraints; revisit thresholds as prevalence changes.
y@&RjjS5-:	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is RPA in healthcare administration?	Robotic process automation automates rules-based tasks like eligibility checks, claims scrubbing, and prior auth status updates to reduce delays and free staff for higher-value work.
Q2Kxv{NNmA	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is NLP used on clinical text?	Named entity recognition, relation extraction, summarization, cohort identification, and coding assistance extract structured signals from notes and reports; quality hinges on domain adaptation.
ME[Ih|Kw$(	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What risks do large language models pose in clinics?	Hallucinations, outdated knowledge, hidden biases, and overconfidence can mislead; safe use requires guardrails, grounding on verified sources, and strict human oversight.
Q(X5,$BNLm	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What makes a good clinical AI use-case?	Clinically meaningful outcome, timely actionability, strong data signal, clear workflow integration, explainability needs met, equity considerations addressed, and measurable ROI/utility.
wC9-sFCnB	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you measure “clinical utility” beyond AUC?	Impact on outcomes, time-to-treatment, length of stay, avoidable admissions, cost, clinician workload, patient satisfaction, and number needed to evaluate/act.
E)h0E|]%f4	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What security threats target AI systems?	Data poisoning, model inversion/membership inference, adversarial examples, prompt injection (for LLMs), and supply-chain compromises; defenses include validation, DP, robust training, and monitoring.
D]%;d5I.m^	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is consent design important for patient trust?	Clear, concise explanations of data use, opt-in/opt-out choices, and meaningful control empower patients and reduce backlash against secondary uses of their information.
J3O?IhQ~ae	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can small hospitals adopt AI responsibly?	Start with narrow, validated assistive tools, use cloud services for scalability, partner with academic/consortia for shared data, and invest in basic data quality and governance first.
s`&8On(78>	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is “human-in-the-loop” in healthcare AI?	A design where clinicians review, confirm, or adjust AI outputs; the system learns from feedback and keeps accountability centered on licensed professionals.
eXUx@?hNkB	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do edge-AI medical devices help?	On-device inference (e.g., wearables, bedside monitors) reduces latency, preserves privacy, and maintains function during outages, enabling continuous, real-time insights.
D5Luk(X7R^	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an example of predictive care in cardiology?	Models forecasting 30-day readmission or acute coronary events from vitals, ECGs, labs, and notes can prompt closer monitoring, medication review, or earlier follow-up.
Jb&:0G6ib?	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you ensure equitable telehealth AI?	Assess device access and connectivity, support multiple languages, design low-bandwidth modes, and monitor performance across age, disability, and socioeconomic groups.
CMyO5n29ag	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is historical bias and why does it persist?	It’s bias baked into past data (e.g., unequal access to care) that models may learn as “signal,” perpetuating inequities unless corrected during problem framing and training.
tx8x!wp(J}	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is algorithmic bias from model design?	When objective functions or features encode unfair trade-offs (e.g., optimizing profit over true need), leading to systematically worse predictions for certain groups.
CDVCftMBMF	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does data auditing reduce bias risk?	By examining representation, missingness patterns, label quality, feature distributions, and proxy variables before modeling, teams can fix issues early rather than after harm occurs.
lvJ~dLK!RT	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What policies help operationalize fairness?	Set fairness targets, monitor subgroup metrics in dashboards, require pre-deployment impact assessments, and include diverse stakeholders in review and escalation paths.
ts<y~-To5e	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Which industry toolkits support fairness and XAI?	Open-source options include Fairlearn, AIF360, What-If Tool, ELI5, Captum, SHAP, and interpretML; they help quantify disparities and surface feature influences.
"eX#.3p3`6}"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the EU AI Act’s relevance to healthcare?	Healthcare decision systems are typically high-risk, triggering requirements for risk management, data quality, transparency, human oversight, robustness, and post-market monitoring.
ibS`kmx2X;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does GDPR affect AI projects with EU data?	Requires lawful basis, purpose limitation, data minimization, rights to access/correct/delete, DPIAs for high-risk processing, and safeguards for automated decisions with significant effects.
pj<5P)lUKl	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What U.S. frameworks touch AI and consumer protection?	Agencies can act against unfair or deceptive practices; state laws like CCPA/CPRA grant transparency and opt-out rights, and sector rules (e.g., HIPAA) govern health data.
"fwv#!8JOEc"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is “AI transparency” in legal terms?	Providing meaningful information about the logic, significance, and consequences of automated decisions so affected individuals and regulators can understand and contest outcomes.
Ci)>yaL8}R	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What careers are growing in AI-for-health?	Data scientist, ML engineer, ML/Ops engineer, clinical informaticist, AI product manager, AI ethicist/auditor, regulatory specialist, biomedical data engineer, and security analyst.
zIXQYBph+:	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What skills do employers seek for these roles?	Solid statistics, Python/SQL, ML frameworks (scikit-learn, PyTorch/TensorFlow), data engineering (Spark), MLOps (Docker/Kubernetes/MLflow), domain literacy, and communication.
Q~CH=u)</?	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Which trends will shape the next five years?	Explainable and trustworthy AI, edge inference in devices, privacy-preserving learning, AI-driven cybersecurity, generative models for discovery, and sustainability-aware ML.
"gUrbbi#0;K"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does AutoML help health teams?	By automating feature/model/hyperparameter search to produce strong baselines quickly, freeing experts to focus on data quality, governance, and clinical integration.
r=1u,7)[V-	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the sustainability angle in AI for health?	Greener training/inference, right-sizing models, efficient hardware, and using AI to optimize facility energy and supply chains reduce carbon footprints while saving costs.
C[Hw;36p/c	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you start an ethical AI project checklist?	Define the clinical question and beneficiaries; map harms; secure consent/legal basis; plan bias testing and explainability; choose an assistive scope; design monitoring and rollback; document everything.
w|(QFz=Q0S	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the bottom line for predictive healthcare?	Predictive healthcare is most impactful when models are accurate, calibrated, fair, explainable, and embedded in workflows with human oversight, rigorous privacy/security, and continuous monitoring.
hq,Jde(^4@	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an accumulator in algorithms?	An accumulator is a variable that’s updated iteratively—usually inside a loop—to “carry forward” a running result such as a sum, product, count, or state. Each iteration reads the old value and writes a new one, enabling step-by-step modeling of change over time.
g<%>3MXV4j	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why do we say modeling + simulation are parallel to the scientific method?	Because you first propose a simplified model (theory), then “experiment” by simulating it, compare simulated outcomes to observations, and refine either the model or the experiment—mirroring hypothesis–test–revise cycles.
JuRwCv0lJ+	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What’s the key difference between for-loops and while-loops	A for-loop runs a predetermined number of iterations (e.g., “simulate 10 years”), whereas a while-loop continues until a condition changes (e.g., “run until population ≤ 0”), which is essential when the stopping time is unknown ahead of time.
sa(y:5Zbt(	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why must the accumulator be initialized before the loop?	The right-hand side of an update reads the accumulator’s previous value; without initialization there’s nothing to read, causing errors or undefined behavior. According to Chapter 4 of Discovering Computer Science, population must be set before applying the yearly update.
IomkXJq@;l	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you generalize the pond simulator to different parameters?	Pass parameters for years, initial population, and annual harvest; then the update becomes population ← (1 + r) × population − harvest so you can vary growth r and removal rates without rewriting the loop.
"p#Qr5AIVi1"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does “don’t crop” mean for domain-aware data prep?	When augmenting or preprocessing images or signals where spatial context is critical, avoid transforms (like random crops) that can remove task-relevant structure. In count-based or time-based simulations, the analog is to avoid discarding state needed for the next step.
ec;ym{8^lt	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are linear, quadratic, and exponential accumulator behaviors?	Linear adds a constant each step (final ≈ a + c·n), quadratic adds a quantity that itself grows linearly with the index (final ≈ Θ(n²)), and exponential adds a fraction of the current value (multiplicative growth, final ≈ a·(1+g)ⁿ). According to Chapter 4 of Discovering Computer Science, these three arise from adding 6, adding the loop index, and adding 0.08×accumulator respectively.
p{]^kj@Fh(	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is accuracy of growth intuition important?	Misclassifying exponential change as linear leads to wildly wrong forecasts (e.g., epidemics, compounding interest, viral adoption), causing over- or under-reaction in planning and policy.
x->OPLM5F~	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does the “network links” example illustrate quadratic growth?	Each newly added node n can link to at most n−1 existing nodes; summing 1+2+…+(n−1) yields n(n−1)/2, a Θ(n²) relationship between nodes and potential links.
y.E<D9@iz,	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you prevent data leakage in iterative simulations?	Only use state from previous steps, set all starting values explicitly, and ensure no step peeks at future information. In practice, keep updates local (read–compute–write) and avoid accidental recomputation that uses already-updated values within the same step.
q]?RI$1T|g	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What should be logged during simulation for reproducibility?	Log inputs (parameters, initial state), iteration counter, accumulator value(s), and any random seeds. This enables exact reruns and post-hoc analysis of where trajectories diverged.
m{Ztix7e2.	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why can accuracy be misleading in growth/decay tasks?	If you only check a final endpoint, you can miss transient dynamics. Evaluate full trajectories (e.g., per-step errors), hitting times (e.g., time to zero), and sensitivity to parameters to ensure models behave well across time, not just at the end.
u$3f2,9pLp	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do format strings help in data reporting?	They align columns and control precision so trends are scannable at a glance (e.g., right-justified years; two decimal places for values), which reduces parsing errors and improves comparison across rows.
pP-$*L}dwu	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What safety checks belong in a while-loop that runs until a condition?	(1) Initialize the condition’s variables before the loop; (2) update them every iteration to make progress toward termination; (3) add a max-iteration guard or timeout to avoid infinite loops when parameters imply no stopping point.
"eB#Fmq<oN|"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why does the ticket-pricing table use iteration instead of calculus?	Brute-force enumeration with a loop is sufficient to profile profit across integer or half-integer prices, and it’s easy to extend to more granular steps or additional constraints without deriving a closed-form optimum.
lW2?OY){!|	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does plotting (e.g., matplotlib) complement tabular output?	Plots reveal shape—peaks, plateaus, inflection points—that tables obscure, enabling quick visual selection of optima and detection of anomalies (e.g., non-monotonic profit curves).
h{v*S[`G~L	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What’s the accumulator pattern’s minimal pseudocode?	Initialize total; loop over steps; update total using its previous value and current inputs; after the loop, return or record total. This skeleton underlies population models, sums, running statistics, and state machines.
nrwbox@lGc	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What common bug arises from updating in-place incorrectly?	Using an updated accumulator value twice within the same step (read–write hazard) can effectively double-apply a rule. The fix is to compute next := f(current) using a temporary, then assign accumulator := next once.
Pk^P)x@_c8	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How would you adapt the pond model to a logistic (capacity-limited) growth?	Replace the linear multiplier with logistic: population ← population + r·population·(1 − population/K) − harvest, where K is carrying capacity; this keeps growth realistic when near resource limits.
y6$(Hb0o%`	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you scan parameter space efficiently (e.g., harvest vs. years)?	Nest loops: outer over candidate harvest values, inner over years updating the accumulator; record metrics such as final population and time-to-extinction to produce contour plots or tables for decision-making.
B1A)FMcGvq	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the benefit of list accumulators?	They store every step’s state (not just the final value), enabling plotting, diagnostics, and post-simulation analysis like detecting oscillations or sudden regime changes.
tZ?M_Y;Z<$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you decide between for vs. while for “time to event” problems?	Use while when the event time is unknown a priori (e.g., “until extinction”). Use for when horizon is fixed (e.g., “simulate 20 years”), possibly combined with an early break if the event occurs sooner.
tM&7e7Q6u0	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is it valuable to print the loop index plus 1 for human-readable years?	Humans typically count years from 1, but loop indices start at 0; adding 1 aligns program output with user expectations and prevents off-by-one interpretation mistakes.
JTC|1>;:[J	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does Caccumulators connect to algorithmic complexity?	According to Chapter 4 of Discovering Computer Science, the same growth classes (linear, quadratic, exponential) that describe accumulator outcomes map to time complexity shapes, reinforcing why exponential-time algorithms become infeasible even for modest input sizes.
e{PE_[84Oj	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a list in Python?	A list in Python is an ordered sequence of items enclosed in square brackets. It can hold any data type, including numbers, strings, or even other lists. Lists are mutable, meaning their contents can be changed after creation.
BUj+G9!@mZ	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are summary statistics?	Summary statistics are numerical values that describe and summarise the main features of a dataset, such as mean, variance, minimum, and maximum values.
y8]g04gG>I	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you compute the mean of a list?	To compute the mean of a list, you sum all elements and divide by the number of elements. For example, mean = total / len(data). If the list is empty, a division by zero error occurs unless handled.
A5;6Mfix]{	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How should an empty list be handled when computing a mean?	An empty list should be handled using a check before division, such as “if len(data) == 0: return None,” to avoid division by zero and indicate missing data.
evrLl31<!X	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an accumulator pattern?	An accumulator pattern is a common algorithmic structure that maintains a running total or collection as a loop processes each item in a sequence, updating a variable each iteration.
Jk_j>c,~=k	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does variance measure?	Variance measures how much data values deviate from the mean, quantifying the spread or variability in a dataset.
tb$T<V=@)z	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can you find the minimum value in a list?	To find the minimum value in a list, initialise the first element as the current minimum, then iterate through remaining elements, updating the minimum whenever a smaller value is found.
h,qf4}`+|E	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the difference between iterating over data and data[1:] when finding a minimum?	Iterating over data[1:] skips the first element, avoiding a redundant comparison between the first item and itself, making the algorithm slightly more efficient.
u},72Am?<t	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can you find the index of the minimum value in a list?	You can find the index of the minimum value by iterating over the indices using range and keeping track of the smallest value’s index. The function returns this index at the end.
lsYSx)pVpw	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can you return the day of the week instead of an index for minimum sales?	You can create a list of days, such as ['Sunday', 'Monday', ...], and use the minimum index as an index into this list to return the corresponding day name.
MfMA;=;V{$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is data smoothing?	Data smoothing is a technique used to reduce noise and reveal patterns by replacing each value in a dataset with the average of nearby values within a defined window.
tr+L)vzd~A	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a moving average?	A moving average is a smoothed representation of data computed by averaging values within a fixed window that slides across the dataset.
zdn+/{Ouyo	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is a moving average used in data analysis?	A moving average is used to reveal underlying trends by reducing random fluctuations or anomalies in the data.
o%bI+&@0q[	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the inefficiency in the naive smoothing algorithm?	The naive smoothing algorithm recalculates overlapping sums repeatedly, performing redundant addition operations for shared elements between windows.
LKukHBuq(`	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can smoothing be made more efficient?	By reusing the previous window’s sum, subtracting the value that leaves the window, and adding the new value that enters, we can reduce redundant operations and achieve faster smoothing.
DrTU%ug@9l	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the computational advantage of the improved smoothing algorithm?	The improved smoothing algorithm reduces arithmetic operations from n(w+1) to 3n + w, performing about w/3 times less work for large window sizes.
tG`>_6Iv{U	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does it mean to modify a list in place?	Modifying a list in place means changing its elements directly without creating a new list object. This alters the original list in memory.
"M4x#&/l7W["	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why are lists mutable?	Lists are mutable because their elements are references to memory locations that can be reassigned, allowing direct modification of contents.
j,F2r4;nfb	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why doesn’t modifying a loop variable change the list?	Modifying the loop variable (like “for rate in unemployment: rate = rate - 0.01”) doesn’t change the list because the variable holds a copy of the value, not a reference to the list element.
C9,Otg&`&	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does passing a list as an argument affect its mutability?	When a list is passed as an argument to a function, changes made to its elements inside the function affect the original list since both reference the same object in memory.
ECA.S|V1ah	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can you modify a list without affecting the original?	To modify a list without changing the original, create a copy using the copy() method and modify the copy instead.
Q,q9OR3$Yd	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why must you separate computing the mean and updating the list during in-place smoothing?	You must separate these steps because updating data[index] too early would overwrite a value needed for computing the next window’s mean.
busL(0j=9V	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the repetition operator (*) do with lists?	The repetition operator (*) creates a new list containing multiple repetitions of the original list’s contents.
c6A6>LEEdj	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the concatenation operator (+) do with lists?	The concatenation operator (+) creates a new list by joining two lists together, unlike append(), which modifies an existing list in place.
MLD!LV0&I2	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the difference between append and concatenation?	append() modifies an existing list by adding one item, while concatenation creates a new combined list, making append more efficient for adding items.
D?TJ2h{x7w	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the sort() method do?	The sort() method rearranges the items in a list into ascending order and modifies the list in place without returning a new list.
l*j&Sv$>l)	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why does assigning sort() to a variable result in None?	Because sort() modifies the list in place and returns None, assigning it to a variable captures this None instead of a sorted list.
KhrS5&z*^L	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What happens if you sort a list with incomparable items?	Sorting a list with incomparable items, such as mixing numbers and strings, raises a TypeError because Python cannot determine their order.
p0qMIRQu22	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the insert() method do?	The insert() method adds an item to a list at a specified index, shifting existing elements to the right.
"Cj&u#0}x}:"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the pop() method do?	The pop() method removes and returns the item at a specified index (or the last item if no index is provided), modifying the list.
CZD<tMKPz!	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why does popping elements in a for loop cause an IndexError?	Popping elements inside a for loop causes IndexError because the list shrinks during iteration while the range of indices remains fixed.
pfZ.GH>Fp>	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does using a while loop fix the pop() IndexError?	A while loop re-evaluates list length each iteration and only increments the index when an element isn’t popped, avoiding skipped or out-of-range indices.
xZv~laRxl6	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the remove() method do?	The remove() method deletes the first occurrence of a specified value from a list and raises a ValueError if the value isn’t found.
RhFH?BANaJ	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a list comprehension?	A list comprehension is a concise syntax for constructing a list in one line using an expression followed by for and optional if clauses.
DU-giLOK4?	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do if clauses work in list comprehensions?	If clauses filter which values are included in the resulting list based on a condition evaluated for each iteration.
Pohkjr7V-K	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a dictionary in Python?	A dictionary is a collection of key-value pairs enclosed in curly braces, allowing fast lookups by unique keys instead of numeric indices.
G9ww(l$F52	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does a dictionary differ from a list?	A dictionary uses custom keys to reference values, while a list uses numeric indices. Dictionaries allow faster lookups and unordered data storage.
c<gK+WVew9	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you add a new key-value pair to a dictionary?	Assign a value to a new key using square brackets, e.g., wordFreqs['new'] = 5.
"FlsFm&#tBM"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can you count word frequencies using a dictionary?	By iterating through a list of words, checking if a word exists as a key, incrementing its count if found, or adding it with a count of one if not.
"mF.K#C>X6N"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the in operator do with dictionaries?	The in operator checks whether a key exists in a dictionary.
rg$[14TE:<	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the keys() method return?	The keys() method returns a special object containing all keys in the dictionary, which can be converted into a list for sorting or iteration.
G{D(9.j(U9	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a hash table?	A hash table is the underlying structure of a dictionary that maps keys to indices (slots) using a hash function, enabling constant-time access.
"foxq(2$#[A"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a collision in hashing?	A collision occurs when two different keys are assigned to the same hash slot. Collisions are managed through secondary hashing or linked lists.
w,!Gpt|M_<	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can you find the most frequent word in a dictionary?	By finding the maximum value in the dictionary’s values, then iterating over keys to collect all that match this maximum frequency.
ejuai:@V)0	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why are common words like “the” removed in text analysis?	Common words are removed because they occur frequently but carry little semantic meaning. Removing them (stop words) highlights more informative terms.
h~*,7*%U{e	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a tuple and how does it differ from a list?	A tuple is an immutable sequence enclosed in parentheses, unlike a list which is mutable. Tuples are used when data should not change.
dCL3@V+VNH	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a bigram?	A bigram is a pair of consecutive words from a text, often used to study word co-occurrence and build language models.
G9IA7<`Wlu	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are bigram frequencies used for?	Bigram frequencies are used in natural language processing for tasks like speech recognition, text prediction, and author style analysis.
kr^LLz!?R0	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is sentiment analysis?	Sentiment analysis is the process of determining whether a text expresses positive, negative, or neutral sentiment, commonly applied to reviews or social media.
bFJD`xyF*@	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does a naive Bayes classifier work in sentiment analysis?	A naive Bayes classifier calculates probabilities of text belonging to sentiment classes based on word frequencies, assuming words occur independently.
w`!hkI_n|	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why are dictionaries used in NLP tasks like sentiment analysis?	Dictionaries provide efficient storage and lookup for word and bigram frequencies, enabling rapid computation of probabilities and feature extraction.
okxDP^>qQq	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the advantage of sorting dictionary items as (value, key) pairs?	Sorting (value, key) pairs allows ordering by frequency while retaining associated keys, enabling ranked output like most frequent words.
vf!7`_)ibw	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why are tuples used as dictionary keys for bigrams?	Tuples are immutable and hashable, making them suitable as dictionary keys, unlike lists which are mutable.
DwkH|TJ6Pf	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the significance of the hash function in dictionaries?	The hash function determines where each key-value pair is stored in the table, allowing fast retrieval and reducing collisions.
hd_qpLOP|l	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is tabular data?	Tabular data is structured data organised in rows and columns, such as spreadsheets or CSV files, commonly used for analysis and visualisation.
o1v:uKA-.f	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is understanding data manipulation important?	Understanding data manipulation enables extracting meaningful insights, supporting evidence-based decisions across scientific and commercial fields.
oOlpYM@o4T	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is two-dimensional data?	Two-dimensional data is data organized into rows and columns, often visualized as a table or grid. Each element is identified by two indices: one for the row and one for the column.
v?&(!}%QQz	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can a CSV file be represented as a two-dimensional structure in Python?	A CSV file can be represented as a list of lists, where each inner list represents one row of the table and each element within that list represents a column value. For example: [['19700101', 33, -294], ['19700201', 83, -261], ['19700301', 122, -139]].
oJ}3p2ouuc	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the advantage of storing data in a table rather than multiple parallel lists?	Storing data in a table (list of lists) keeps related values together, improving readability, reducing indexing errors, and making it easier to perform operations on rows or columns as single entities.
kkVY&oMS3c	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can you access the value in row r and column c of a two-dimensional list?	You can access the value at row r and column c using double indexing: table[r][c].
Mg/nymR_Bk	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the purpose of the getMinTemp function in Chapter 8?	The getMinTemp function searches a two-dimensional table for a given date and returns the corresponding minimum temperature. It demonstrates how to use both row and column indices to locate specific data points.
O&),s9J|Mv	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can the number of columns in a two-dimensional table be found?	The number of columns can be found by measuring the length of any row, for example: len(table[0]).
HG:IuK2xS`	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is pandas and how does it relate to tabular data?	Pandas is a Python module used by data scientists to analyze tabular data efficiently. It provides a data structure called a DataFrame that allows easy reading, filtering, and manipulation of CSV files and other tabular datasets.
gB0K_sD}Iy	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the difference between a list of lists and a pandas DataFrame?	A list of lists is a basic Python structure requiring manual indexing, while a pandas DataFrame is a specialized data structure offering named columns, fast indexing, filtering, and built-in statistical operations.
j;G6khfdGH	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can you select a specific cell from a pandas DataFrame using an index?	You can select a specific cell using the .at property, for example: temps.at[19700201, 'EMNT'].
s}fip8GwM]	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a cellular automaton?	A cellular automaton is a grid-based simulation where each cell changes its state according to rules based on the states of its neighboring cells. Each update step represents a new generation in the simulation.
hFW$cnTW+w	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is Conway’s Game of Life?	Conway’s Game of Life is a cellular automaton in which each cell can be alive or dead, and its next state depends on the number of live neighbors it has. The system evolves according to simple rules that produce complex, emergent behaviors.
Q6FC!:lL.7	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the four rules of Conway’s Game of Life?	The rules are: (1) a live cell with fewer than two live neighbors dies of loneliness, (2) a live cell with two or three live neighbors survives, (3) a live cell with more than three live neighbors dies of overcrowding, and (4) a dead cell with exactly three live neighbors becomes alive.
Q|Ze~,/0Zp	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a glider in the Game of Life?	A glider is a small pattern of cells that repeats its shape as it moves diagonally across the grid over successive generations.
DiaU/K86jg	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is a two-dimensional grid represented in Python?	A two-dimensional grid is represented as a list of lists, where each inner list represents one row of cells. Each cell can be represented by a value such as 0 for dead and 1 for alive.
J1(paGRLQv	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can a grid of zeros be easily created in Python?	A grid of zeros can be created using the list repetition operator inside a loop. For example: grid = [[0] * columns for r in range(rows)].
"J2z9U#vM)O"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the purpose of the initialize function in the Game of Life program?	The initialize function sets specific coordinates in the grid to the ALIVE state based on a list of (row, column) tuples representing initially live cells.
bPs;:;n_mm	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How are neighboring cells determined in a cellular automaton?	Neighbors are determined using coordinate offsets relative to the current cell, such as (-1, -1), (-1, 0), ..., (1, 1). The neighborhood function iterates over these offsets to count the number of live neighbors.
Q-9Z/.1H<e	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why must we check if neighbor coordinates are valid?	We must check that each neighbor coordinate lies within the bounds of the grid to avoid indexing errors when accessing cells on the edges or corners.
m}~L`ehU-n	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What problem occurs if we update cells directly during iteration in the Game of Life?	If we modify cells directly, subsequent neighbor counts may use updated rather than previous-generation values, leading to incorrect behavior. The solution is to update a copy of the grid.
Af]vH1_t3,	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is deepcopy used in updating the grid?	The deepcopy function creates an independent copy of the grid so that updates can be made safely without affecting the original grid during neighbor evaluation.
"lZ#r4N*^Df"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the purpose of the life function?	The life function simulates the Game of Life for a specified number of generations, updating the grid according to the rules and returning the final configuration of cells.
tFJ&~ruki*	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is NumPy useful for working with two-dimensional data?	NumPy provides efficient array operations that allow mathematical and logical operations to be applied to entire grids or matrices in a single statement, improving speed and readability.
"sl#[xqK;yz"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does a NumPy array differ from a list of lists?	A NumPy array is stored in contiguous memory and supports vectorized operations, while a list of lists is a nested structure that requires explicit loops for most operations.
nkt>.m|1H$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a digital image in computing terms?	A digital image is a two-dimensional grid of pixels, where each pixel contains color or brightness information represented by numeric values.
Q$~f]H$`H/	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a pixel?	A pixel, short for “picture element,” is the smallest unit of a digital image representing one color value.
P%0UX}$5Uz	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the difference between grayscale and color images?	A grayscale image uses one channel to represent brightness levels, while a color image uses three channels (red, green, and blue) to represent full color.
q}o5D.@Qr(	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the RGB color model?	The RGB color model represents colors by combining red, green, and blue light intensities. (0,0,0) is black, (255,255,255) is white, and other combinations represent different colors.
o8IsyP{-_$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the CMYK color model?	The CMYK color model (cyan, magenta, yellow, black) is a subtractive model used in printing, where colors are created by absorbing light rather than emitting it.
bSteh>V]4$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an alpha channel?	An alpha channel represents transparency. A value of 0 means fully transparent, 255 means opaque, and intermediate values indicate partial translucency.
tFTA=Aoka2	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How many distinct colors can be represented with 8 bits per channel?	With 8 bits per channel, there are 2^24 = 16,777,216 distinct color combinations.
lR;3TGW-B_	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What RGB values represent pure blue, purple, and teal?	Pure blue is (0,0,255), purple is (128,0,128), and teal is (0,128,128).
js9E`wTkUk	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an image filter?	An image filter is an algorithm that modifies or enhances an image by processing each pixel, such as converting it to grayscale, blurring, or adjusting brightness.
cj8uBv9n{k	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is brightness computed from an RGB color?	Brightness can be computed as the average of the red, green, and blue components, for example (R + G + B)/3.
Ho-p],(_7Q	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the color2gray function do?	The color2gray function converts an RGB color to grayscale by averaging its three color channels and returning an equivalent gray RGB tuple.
P-e/kHrwH+	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are BMP, GIF, and JPEG file formats?	BMP stores raw pixel data with minimal compression; GIF uses indexed colors and lossless compression; JPEG uses lossy compression to reduce file size while maintaining visual quality.
"K$A#l>Ng(Q"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the main difference between lossless and lossy compression?	Lossless compression preserves all original data exactly, while lossy compression removes less noticeable information to reduce file size.
eO4Z!&6x?&	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the grayscale function do in the image module?	The grayscale function iterates over every pixel in an image, converts each color to grayscale using color2gray, and returns a new grayscale image.
x02H<zvShB	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the purpose of the rotate90 function?	The rotate90 function rotates an image 90 degrees clockwise by moving each pixel from position (x, y) in the original to position (h - 1 - y, x) in the rotated image.
I4Kn&pPX8	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What transformation rule is used when rotating an image 90 degrees clockwise?	Each pixel at position (x, y) in the original image moves to position (h - 1 - y, x) in the rotated image, where h is the image height.
r]G&BjhAPy	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the reduce function do?	The reduce function decreases the size of an image to one quarter of its original area by taking every second pixel in both dimensions, producing an image half as wide and half as tall.
b$f)T)JzBS	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is averaging color values better than discarding pixels when reducing an image?	Averaging color values produces smoother results by preserving more visual information from the original image instead of arbitrarily keeping or discarding pixels.
u&G.Blzuv;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are some ways to combine image filters?	Image filters can be applied sequentially—for example, first reducing an image, then converting it to grayscale, and finally rotating it—to create customized effects.
BvR~@x!J6[	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the broader significance of two-dimensional structures in computing?	Two-dimensional structures are fundamental for representing and manipulating tabular data, simulations, and images. They enable complex modeling tasks such as cellular automata, image processing, and data visualization.
n(455L%lyw	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the main takeaway from Chapter 8 of Discovering Computer Science?	According to Chapter 8 of Discovering Computer Science, two-dimensional data structures such as tables, grids, and images are essential tools for organizing information and modeling complex systems, forming a bridge between abstract computation and real-world applications like simulation and image analysis.
K!MR*%~I_n	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why does modern data scale demand new approaches?	The world produces massive, fast-growing volumes and varieties of data, so storage, processing, and analysis must be efficient and privacy-aware, often requiring alternative sources or synthetic generation when real data are scarce or restricted.
dCw7v3@}vL	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the difference between qualitative and quantitative attributes?	Qualitative attributes describe categories or labels and are observed or compared (e.g., color, city), while quantitative attributes are numeric measures on which statistical and visualization methods can be directly applied (e.g., height, price).
"wr#Kfh5od*"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a nominal attribute?	A nominal attribute is categorical and unordered (e.g., Red/Blue/Green); only equality comparisons make sense, and values can’t be ranked meaningfully.
yB.>{{8Wfw	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a binary attribute and what do symmetric vs. asymmetric mean?	A binary attribute has exactly two mutually exclusive states (e.g., 0/1, True/False). Symmetric means both states have equal importance; asymmetric means one state is more informative or rare (e.g., “Present” vs. “Absent” in attendance).
I0my*7h%w%	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an ordinal attribute?	An ordinal attribute has values that can be ranked (e.g., Good/Better/Best), but differences between adjacent ranks are not necessarily equal or known.
m?MKd4=$z^	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are discrete vs. continuous quantitative attributes?	Discrete attributes are countable whole numbers (e.g., number of cars), whereas continuous attributes can take on any value within a range (e.g., weight, price), often represented with fractions or floats.
k,VF^>c<[l	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the difference between interval and ratio data?	Interval data support ordered differences but lack a true zero (e.g., temperature in °C), while ratio data have ordered differences and a meaningful zero, supporting meaningful ratios (e.g., 4 is twice 2).
"nCNQ#^[NZV"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why are “Family Dependents” ratio data in this chapter?	According to Chapter 2 of Fundamentals of Data Science, “Family Dependents” have a meaningful zero (no dependents) and support ratio statements (e.g., 3 is 50% more than 2), so they fit the ratio scale.
xw]{X(LlNY	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a data structure (in the context of storage formats)?	A data structure is a way to organize and store data so it can be accessed and updated efficiently, preserving logical relationships and supporting operations like retrieval and update.
i5Ez4QhfAE	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is structured data?	Structured data follow a predefined schema (e.g., rows and columns in a relational table or spreadsheet), enabling efficient indexing, querying, and joining.
Qzd{4O2QQO	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is unstructured data?	Unstructured data have no fixed schema and include free text, images, audio, and video; they often need specialized processing (e.g., NLP, computer vision) and are commonly stored in NoSQL or object stores.
t0KaChN8$;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is semistructured data?	Semistructured data don’t conform to rigid schemas but carry tags or hierarchical markers (e.g., JSON, XML, email headers) that provide partial structure for parsing and analysis.
lc<>soAez&	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are primary data sources?	Primary sources provide first-hand, original data collected directly from instruments or studies (e.g., IoT sensors, clinical surveys), offering high authenticity but often at greater cost and effort.
C$I7FRg-8:	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are secondary data sources?	Secondary sources curate and publish data derived from primary sources, often cleaned and documented for reuse—useful for rapid validation and benchmarking.
NKXzfHo[C,	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the risk of misusing secondary data?	Even high-quality secondary data can be mismatched to a task (e.g., using general network datasets to validate an IoT IDS), leading to misleading performance estimates.
mD[*5Vl9V|	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Which popular repositories are listed in the chapter?	According to Chapter 2 of Fundamentals of Data Science, examples include the UCI ML Repository (≈622 curated tabular datasets), Kaggle (≈77k datasets and competitions), Awesome Public Datasets (GitHub-curated), NCBI (biomedical databases), SNAP (large network datasets), MNIST/EMNIST (handwritten characters), VoxCeleb (speaker recognition), and Google Dataset Search (≈25M indexed datasets).
gh(S-UWd{f	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the difference between homogeneous and heterogeneous data sources?	Homogeneous sources have similar types and formats and are easier to integrate (e.g., the same store schema across locations). Heterogeneous sources vary in type/format and typically require preprocessing and transformation before integration.
I6pu5PE`8`	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What kinds of heterogeneity are identified?	According to Chapter 2 of Fundamentals of Data Science, four forms are outlined: syntactic (different formats/languages), conceptual/semantic (different data models in the same domain), terminological (different names for the same thing), and semiotic/pragmatic (different interpretations by users).
s!pS0hQuR9	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is synthetic data?	Synthetic data are artificially generated records that mimic the statistical properties of real data, enabling experimentation when real data are unavailable, restricted, or privacy-sensitive.
i9GC+d.&/Z	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does synthetic data differ from data augmentation?	Data augmentation creates modified copies of existing samples (e.g., image rotations) to expand a dataset; synthetic data create new samples—possibly from scratch—matching learned distributions rather than editing individual instances.
O1|496]qhH	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does randomization differ from synthetic data?	Randomization perturbs existing data to obfuscate or protect sensitive information, whereas synthetic generation produces new records that resemble real data without directly exposing their original values.
"qF/U#Q!Vg%"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are key advantages of synthetic data?	Lower cost/time vs. collecting real data, built-in labels for training, scalability to rare scenarios, controllable distributions, and improved privacy by reducing exposure of real records.
vO*tyBpw^$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are fully vs. partially synthetic data?	Fully synthetic data contain no direct copies of real records (all values generated), while partially synthetic data replace only sensitive fields with generated values, retaining the rest of the record.
Cz|E]a6M=t	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the main steps in a synthetic-data workflow?	According to Chapter 2 of Fundamentals of Data Science, core steps include determining objectives and constraints, selecting a generator model, collecting real samples (if applicable), training the model, generating new data, and evaluating the synthetic data by training models and testing against real data.
u]W2x&s^>M	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is synthetic data quality evaluated?	A practical approach is to train target analytics on synthetic data and test on real holdouts; if performance meets expectations, the generated data are considered fit; otherwise, tune the generator and repeat.
AmMNs<2F)v	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What statistical methods can generate synthetic data?	According to Chapter 2 of Fundamentals of Data Science, you can fit and sample from distributions (e.g., Gaussian, chi-square), using expert judgment when samples are scarce and carefully managing sampling bias.
BPbPMV+MBB	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is agent-based modeling for data generation?	Agent modeling simulates entities (agents) behaving like real-world actors; you calibrate agents (possibly via curve-fitting or decision trees), then run simulations (e.g., Monte Carlo) to produce realistic synthetic traces.
MpZ`u]4sUD	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Which deep models are highlighted for generation?	According to Chapter 2 of Fundamentals of Data Science, Variational Autoencoders (VAEs) learn latent distributions to reconstruct and sample like the originals, and Generative Adversarial Networks (GANs) pit a generator against a discriminator to produce highly realistic samples.
zn<.Nz/W?$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When is synthetic data preferable to real data?	It’s especially helpful when privacy, legal, or access constraints block use of real data; when edge cases are rare; or when rapid iteration at scale is needed before costly data collection.
P!fp|XFmP-	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are risks or limitations of synthetic data?	If the generator learns an incomplete or biased distribution, synthetic data may omit rare but important patterns, distort correlations, or fail to transfer model performance to real-world settings.
x}e^hXil.;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Which sector-specific synthetic data tools are mentioned?	According to Chapter 2 of Fundamentals of Data Science, examples include Hazy, Datomize, Mostly.AI, and Facteus (banking/fintech); MDClone and SyntheticMass (healthcare); CVEDIA SynCity and Rendered.AI (computer vision/simulation).
MOTY:KP`(n	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Which Python libraries can generate synthetic or fake data?	According to Chapter 2 of Fundamentals of Data Science, options include scikit-learn (classification/regression/clustering generators), NumPy (random sampling from distributions), Pydbgen/Mimesis/Faker (fake tabular data), SymPy (symbolic-expression datasets), and SDV (single-table, relational, and time-series synthesis).
AF|59LZdTc	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does Table 2.5 highlight about libraries?	According to Chapter 2 of Fundamentals of Data Science, it maps libraries to tasks—for example, SDV/time-series generators for temporal data, Mesa for agent-based modeling, Blender/Zpy for 3D/computer-vision scenes, and DataSynthesizer for simulating from a given dataset.
hYM6IG&okV	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How should I pick a generator library or tool?	Match the library to the data modality (tabular/time-series/images/graphs), consider privacy needs, and weigh customization vs. ease-of-use: libraries enable code-level control; turnkey tools favor simplicity but may limit configuration.
PcBK,P<~?A	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What makes a primary source “better” than a secondary one?	Primary data can be tailored to the exact question and offer traceable provenance, but they’re costly; secondary data are faster and cheaper but may be mismatched or preprocessed in ways that limit your use.
rKrZ3)*Z<~	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the first thing to define before generating data?	You should explicitly determine objectives, constraints, and privacy/compliance requirements so the generator can be chosen and evaluated against the right success criteria.
pd08H3;056	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is sample collection important for generation?	High-quality real samples help the generator approximate true distributions; poor or biased samples lead to synthetic data that fail to generalize.
f5Zdpu`1,M	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do VAEs and GANs differ in practice?	VAEs optimize a reconstruction-plus-regularization objective to learn smooth latent spaces; GANs train adversarially to fool a discriminator, often producing sharper but potentially less stable outputs.
L]k7Y~bi4n	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What’s a simple example of statistical synthesis?	If you estimate that widget demand is normally distributed with mean 100 and standard deviation 15, you can sample thousands of synthetic weekly demands from N(100, 15²) to stress-test inventory policies.
p&rEQ`+w-C	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does heterogeneity complicate integration?	Differing formats, schemas, meanings, labels, and user interpretations require cleaning, normalization, schema mapping, ontology alignment, and sometimes lossy transformations.
GQ3a~Dsqa-	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What role do JSON and XML play in semistructured data?	They provide tagged, hierarchical containers that are self-describing, making it easier to parse, validate, and transform semistructured records such as emails, API responses, or logs.
OYEv.=|i>F	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is KDD vs. data mining in this chapter’s framing?	Knowledge Discovery in Databases (KDD) is the broader process of extracting useful knowledge from large data, and data mining is the intermediate step focused on discovering hidden patterns that support that knowledge.
BCF1>KhBQz	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How much data generation does the chapter cite?	According to Chapter 2 of Fundamentals of Data Science, examples include ~2.5 quintillion bytes produced daily, Facebook generating ~4 PB/day, Google handling ~40,000 searches/second, and Google Dataset Search indexing ~25 million datasets.
c+e,QKfq^H	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why do organizations turn to synthetic data for privacy?	Synthetic records can reduce exposure of real personal data while maintaining utility for modeling, helping teams experiment and share data under privacy, IP, or regulatory constraints.
i:KNZuHJeu	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you know if synthetic data are “good enough”?	They’re “good enough” when models trained on them perform acceptably on real validation data and when key statistical properties (marginals, correlations, rare-event rates) align with requirements without leaking sensitive information.
cF/89C.DTn	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is data preparation necessary even when models are advanced?	Advanced models can’t compensate for missingness, noise, inconsistencies, or mismatched distributions; careful preparation improves reliability, reduces bias, and aligns data with model assumptions so downstream analyses are trustworthy.
dek4u&+b:c	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does “data quality” mean in this chapter?	According to Chapter 3 of Fundamentals of Data Science, data quality is judged by completeness, correctness, and consistency; improving these through cleaning makes data fit for purpose.
O!o+G(>)Hi	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the main preprocessing stages outlined?	According to Chapter 3 of Fundamentals of Data Science, the major stages are Data Cleaning (handle missingness/duplicates), Data Reduction (sampling/dimensionality reduction), Data Transformation (discretization/binning, scaling), Data Normalization (min–max, z-score, decimal scaling, quantile, log), and Data Integration (consolidation/federation).
L$QtNHEsQc	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What kinds of real-world challenges produce “messy” data?	Instrumentation failures, transmission interruptions, heterogeneous collection standards, inconsistent normalization, and human or pipeline errors all create missing values, noise, and format inconsistencies that require preprocessing.
JPMHBKCuzn	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What missing-data mechanisms are discussed?	According to Chapter 3 of Fundamentals of Data Science, the chapter distinguishes MCAR (missingness unrelated to any values), MAR (depends on other observed variables), and MNAR (depends on the missing value itself).
g67lSAYYP1	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When is it acceptable to drop missing data?	It’s reasonable under MCAR, small proportion of loss, and when the dropped cases won’t bias results; otherwise, discarding can distort distributions or reduce power.
K8c`Tv~)Q[	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is complete-case analysis?	Complete-case analysis removes any record with a missing value and analyzes the remainder; it’s simple but can bias results unless data are MCAR.
Oq|RJ-,.M!	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is attribute discarding and when would you use it?	Attribute discarding drops variables with extensive missingness and low relevance; if the attribute is critical, it should be retained and imputed instead.
r>F8XGnGZ*	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What approaches does the chapter give for MNAR?	According to Chapter 3 of Fundamentals of Data Science, MNAR often requires parameterized modeling (e.g., EM with maximum likelihood, BPCA variants) that explicitly estimates the missingness structure.
dGuqQd3C8w	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is imputation in one sentence?	Imputation replaces missing entries with plausible values inferred from observed data to restore completeness while attempting to preserve statistical properties.
O8lu-J7{|K	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do mean, regression, LS/LLS, and KNN imputations differ?	According to Chapter 3 of Fundamentals of Data Science: mean imputation is simple but shrinks variance; regression imputation predicts from other variables (risking inflated correlations); LS/LLS use least-squares fits (LLS uses KNN first to find similar cases); KNN imputes from local neighbors based on distance.
l{UDc~:j.]	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are hot-deck and cold-deck imputations?	Hot-deck uses donors from the same dataset (e.g., last observation carried forward); cold-deck uses donors from an external dataset; both avoid model estimation and keep realistic values.
bf2-Q4Rl!Z	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is multiple imputation?	Multiple imputation generates several plausible datasets by modeling missingness (e.g., via chained regressions or propensity-score groups), analyzes each, and combines results to reflect uncertainty.
9@%+x|}cK	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why detect duplicates before modeling?	Duplicates inflate storage and compute and can bias learning; removing them speeds pipelines and yields more representative statistics.
oT[g(RTNLG	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What duplicate-removal strategies are outlined?	According to Chapter 3 of Fundamentals of Data Science, the chapter discusses knowledge-based cleaning (e.g., standardizing domain abbreviations, rule-based merges) and ETL workflows that clean at instance and schema levels.
DBSkFP5fUY	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is ETL in this context?	ETL means Extract, Transform, Load—a pipeline that pulls data from sources, cleans/transforms it (including de-duplication), and loads it into a warehoused schema for analysis.
o>3ZU1%yCF	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the goal of data reduction?	Reduce volume or dimensionality so analysis is faster and cheaper while preserving the information content relevant to the task.
l4GMduN9xG	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do parametric reduction and nonparametric summaries differ?	Parametric methods fit a compact model and keep only parameters (and outliers); nonparametric summaries avoid explicit models and use sample statistics or subsets to represent data.
x=He]:S2b<	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What sampling schemes are described?	According to Chapter 3 of Fundamentals of Data Science: simple random sampling with/without replacement, stratified sampling (sample within strata), and cluster sampling (sample clusters then elements).
by]B[Y6U-z	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why prefer stratified or cluster sampling on skewed data?	They ensure coverage of important subpopulations (strata) or reduce cost by sampling groups (clusters), often yielding more stable estimates than naive random sampling.
jPci5]nt7s	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is dimensionality reduction trying to avoid?	It mitigates the curse of dimensionality, sparsity, and distance unreliability that degrade algorithms like clustering and nearest neighbors on high-dimensional data.
w;AP6H@pKs	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does PCA reduce dimensions?	According to Chapter 3 of Fundamentals of Data Science, PCA finds orthogonal directions (principal components) that explain maximum variance and projects data into a lower-dimensional subspace with minimal information loss.
hY@lOV;Lr[	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is data transformation and why do it?	Transformation changes data’s form to meet statistical assumptions or improve interpretability—for example, binning continuous variables or mapping scales to stabilize variance.
M=uMDMMghd	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is discretization (binning) and its benefits?	Discretization converts continuous attributes into intervals, reducing memory, easing visualization, and sometimes improving model accuracy (e.g., with rule learners) by simplifying value ranges.
Nr03{A4/sh	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are “cutpoints” and “arity” in discretization?	Cutpoints are values that split a continuous range into bins; arity is the number of resulting bins (cutpoints + 1), which trades interpretability against model granularity.
EA<S&(avY$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Which unsupervised discretizers are listed?	According to Chapter 3 of Fundamentals of Data Science: average/midrange splits (simple but outlier-sensitive), equal-width binning, equal-frequency binning, and k-means-based discretization.
Ar$H)!hf3m	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Which supervised discretizers are listed?	According to Chapter 3 of Fundamentals of Data Science: entropy-based schemes (e.g., MDLP), chi-square merges (ChiMerge/Chi2), and other dependency- or wrapper-based methods (e.g., CAIM, OFFD, Ameva, Zeta, CADD).
Ej<4XK.pcs	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is normalization vs. transformation in this chapter’s usage?	Normalization is a specific form of transformation aimed at shaping distributions (often toward normality) or scaling for comparability across features.
qDRA+zYAzo	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does min–max normalization work?	It linearly rescales a feature to a fixed range (commonly [0,1]) using the column’s observed min and max so all values become comparable in magnitude.
g?F?K<+voj	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When should I use z-score normalization?	Use z-scores when centering and scaling by mean and standard deviation help models assume standardized inputs (mean 0, unit variance), aiding comparability and many linear methods.
iAOP)b..q^	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is decimal-scaling normalization?	It shifts the decimal point by a power of 10 chosen so the maximum absolute value falls within (−1,1), providing simple magnitude normalization.
u>rh}1R~[P	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is quantile normalization used for?	According to Chapter 3 of Fundamentals of Data Science, quantile normalization forces multiple columns to have identical empirical distributions by ranking values and replacing them with averaged rank values; qsmooth relaxes this when groups legitimately differ.
xwEhkW(M<B	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why apply log normalization?	Log transforms reduce right-skew and heteroskedasticity, making multiplicative effects additive and stabilizing variance—common in omics and psychosocial measures.
E<LJ(v!@B3	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is data integration and why do it?	Data integration combines heterogeneous sources into a usable common view so analyses reflect broader evidence and reduce single-source bias.
MmuL*>KE+V	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the three integration levels?	According to Chapter 3 of Fundamentals of Data Science: data-level (unified storage/query), processing-level (methods that exploit cross-dataset correlations), and decision-level (ensembling separate model outputs).
nv;jBqXHYV	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What’s the difference between ETL and ELT in integration?	ETL transforms before loading into the destination; ELT loads first and transforms in place at the destination engine—both are valid orchestration patterns in this chapter’s framing.
vFQvN!Ew9H	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are tight vs. loose coupling?	According to Chapter 3 of Fundamentals of Data Science, tight coupling moves all sources into a consolidated store (ETL/ELT), whereas loose coupling federates queries across sources on demand without centralizing data.
j[Y`DE3|nE	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is consolidation in practice?	Consolidation centralizes multi-source data in a destination store for downstream analytics; it offers a single source of truth but must manage latency and refresh.
Ri>M5F_$s:	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is federation in practice?	Federation creates a virtual unified view that pushes queries to underlying heterogeneous systems at request time, avoiding data movement but relying on on-the-fly translation and source availability.
pIB7TyP$,;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How should I sequence preprocessing steps?	A practical sequence is: profile data → clean missing/duplicates → transform/normalize as needed → reduce (sampling/PCA) to match modeling requirements → integrate sources (consolidate or federate) → re-profile to verify assumptions still hold.
e7DCQJw@<k	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Define machine learning in one sentence?	Machine learning is a way of building programs that improve at a task by learning patterns from data rather than following only hand-written rules; formally, as stated in Chapter 4 of Fundamentals of Data Science, a program “learns” if performance P on task T improves with experience E.
PA>KYY2-{.	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is machine learning different from explicit programming?	In explicit programming, a developer encodes rules that map inputs to outputs; in machine learning, the system is given input–output examples and learns the rules (a function) that best map inputs to outputs, adjusting parameters when predictions are wrong.
jFJ_&uz,M!	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the three main learning paradigms?	The three paradigms are supervised learning (learn from labeled examples), unsupervised learning (discover structure from unlabeled data), and semi-supervised learning (learn from a small labeled set plus a large unlabeled set).
ygWJyGkm|c	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is supervised learning in practical terms?	Supervised learning learns a function that maps feature vectors to target labels so future, unseen items can be classified or predicted based on labeled training data.
Qcf>F1uK./	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Formalise supervised learning.	According to Chapter 4 of Fundamentals of Data Science, supervised learning learns a function f:X→Y from labeled pairs (xi,yi) so that for a new x the model predicts ŷ=f(x) using patterns learned from the training set.
kl!ZF0&Z*?	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is unsupervised learning trying to accomplish?	Unsupervised learning searches for structure—such as clusters or representations—using only the input features, aiming for high similarity within groups and lower similarity across groups.
t|jAy&N;C]	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Formalise unsupervised learning.	According to Chapter 4 of Fundamentals of Data Science, unsupervised learning finds a mapping g:X→C that assigns inputs to clusters or representations without labels, revealing inherent relationships in the data.
lxo]p-sC9l	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When is semi-supervised learning useful and how does it proceed?	Semi-supervised learning is useful when labeled data are scarce and unlabeled data are abundant; it trains on the labeled subset, uses the model to label high-confidence unlabeled examples, adds them to training, and iterates with a confidence threshold to avoid propagating errors.
rw,e0,Ch.s	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the recommended dataset split and why?	A common practice is to split preprocessed data into training (to fit parameters), validation (to tune hyperparameters and prevent overfitting), and test (a final, untouched estimate of generalization) so evaluation remains unbiased.
J&EfI0cc{I	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does K-fold crossvalidation work?	K-fold crossvalidation partitions data into K equal folds, trains on K−1 folds and tests on the held-out fold, repeats this K times with different folds as the test set, and averages results to reduce variance from a single split.
ktd:QS:Azs	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why shouldn’t models be tuned on the test set?	Tuning on the test set leaks information, causing the model to overfit to that set and invalidating the final, unbiased estimate of real-world performance.
g;M*n2&q`A	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is inductive bias and why does it matter?	Inductive bias is the set of assumptions a learner uses to choose one function among many that fit the data; it matters because different biases (e.g., linear separators vs. rectangles) fit different problem structures and affect generalization.
hH+`)sThaW	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What examples of inductive bias does Chapter 4 highlight?	According to Chapter 4 of Fundamentals of Data Science, examples include learners biased to: (a) linear/hyperplane separators, (b) axis-parallel splits, (c) axis-aligned rectangles, and (d) maximum-margin separators; clustering biases like spherical (K-means), grid-based, or arbitrary-shape discovery are also discussed.
I%OsWIW?C5	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does “model generalization” mean?	Generalization is a trained model’s ability to make accurate predictions on new, unseen data by capturing true underlying patterns rather than memorizing training specifics.
uy$TsxY2YX	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is underfitting and how can it be mitigated?	Underfitting happens when a model is too simple or the data/feature signal is too weak to capture true structure, leading to poor training and test performance; adding informative features, using more expressive models, or collecting more data can mitigate it.
zr2@hmWS[;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is overfitting and how can it be controlled?	Overfitting occurs when a model captures noise/idiosyncrasies in training data and performs poorly on new data; you can control it with proper train/validation/test splits, K-fold CV, early stopping, regularization, and by limiting model capacity.
csU)(8^o~j	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why might “some training error” be desirable?	Allowing a modest training error often improves generalization because a perfectly fit training set may indicate that the model has learned noise rather than signal.
Qru9aQ8=w>	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How should you handle unbalanced classes during training/evaluation?	Use stratified sampling, class-aware metrics (precision/recall/F1), resampling (oversample minority or undersample majority), or generate plausible minority examples; Chapter 4 notes that decision trees sometimes fare well directly and that VAEs/GANs can aid data augmentation for images.
ix2;]%=AST	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are TP, FP, TN, and FN?	True Positives (TP) are positives correctly predicted positive; False Positives (FP) are negatives incorrectly predicted positive; True Negatives (TN) are negatives correctly predicted negative; False Negatives (FN) are positives incorrectly predicted negative.
M.^6xvOXw-	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a confusion matrix and how do I read it?	A confusion matrix is a table where rows typically represent actual classes and columns represent predicted classes; the main diagonal counts correct predictions per class, while off-diagonals show specific confusions.
h>O4(Hm8-;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is classification accuracy defined and when can it mislead?	Accuracy is (TP+TN)/(TP+FP+TN+FN); it can be misleading on imbalanced data where predicting the majority class yields high accuracy but poor minority detection.
K^@_gyX!<&	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does precision measure in plain language?	Precision answers “of the items predicted as positive, how many were actually positive,” so it reflects false-alarm control.
y&7N4UtVx%	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does recall measure in plain language?	Recall answers “of the actual positives, how many did we correctly identify,” so it reflects miss-rate control.
zk6;DxHqZI	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the F1-score and why use it?	The F1-score is the harmonic mean of precision and recall, giving a single number that balances false positives and false negatives—handy when classes are imbalanced or when both types of error matter.
q;Hw{S~aEI	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do macro-average and micro-average differ?	Macro-averaging computes each class’s metric (precision/recall/F1) and averages them, giving all classes equal weight; micro-averaging pools TP/FP/FN across classes first, giving more weight to frequent classes and often suiting imbalanced data.
"lt3nG$I6#u"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Which averaging should one report?	Report both when possible: macro to show per-class fairness and micro to reflect overall instance-level performance; choose the one aligned with your application’s goals if you must pick one.
vDCMOSE+lz	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How should experiments be designed and reported?	According to Chapter 4 of Fundamentals of Data Science, you should obtain or collect labeled data, preprocess, split into train/validation/test or use K-fold CV, tune on validation only, test once, and report clear metrics (e.g., per-class and averaged scores), optionally benchmarking on public repositories like UCI or Kaggle.
C!1TP}ckw7	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why use hypothesis tests like the t-test or ANOVA in ML results?	They help determine whether observed differences between algorithms or configurations are statistically significant rather than due to random chance across runs/datasets.
c>gE70^EV_	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When is a t-test appropriate for comparing models?	A t-test compares the means of a metric (e.g., F1 across folds/runs) from two groups when normality and equal-variance assumptions are reasonable; a significant t suggests the two settings differ meaningfully.
jkfpq.Bt>h	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When should one use ANOVA?	Use ANOVA to compare means across three or more groups (e.g., several algorithms or hyperparameter settings); a significant F-statistic indicates at least one group differs, prompting post-hoc comparisons.
g;R|RcOa2G	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do training, validation, and testing roles differ?	Training adjusts model parameters to fit data; validation guides hyperparameter choice and early stopping; testing provides a one-time, unbiased estimate of performance on unseen data.
l.g;g+51NS	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What practical steps reduce leakage and bias during evaluation?	Keep the test set untouched until final evaluation, perform all data transforms (e.g., scaling) inside crossvalidation folds or fitted on training only, stratify splits for class balance, and ensure no duplicate or near-duplicate records span train/test.
t,W~XVraJV	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is early stopping in simple terms?	Early stopping halts training when validation performance no longer improves, preventing the model from fitting noise that would harm generalization.
"k4,wvM#/(8"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is regularization and what does it buy you?	Regularization adds a penalty for model complexity or large weights to the learning objective, discouraging over-complex fits and improving generalization.
k1q5tt^o<^	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How should one choose between precision-oriented vs. recall-oriented tuning?	Base it on the cost of errors: if false alarms are costly (e.g., spam flagged as important), favor precision; if misses are costly (e.g., medical screening), favor recall; use F1 or a cost-sensitive metric to balance both.
m!.k>8$eJ6	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is regression in data science?	In data science, regression is a supervised learning approach used to model the relationship between one or more independent variables (features) and a continuous dependent variable (target), allowing predictions of numeric outcomes for unseen examples.
"N,NU#yc1EP"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the main goal of regression analysis?	The main goal of regression analysis is to learn a function that best captures how changes in independent variables affect the dependent variable, while generalizing well to new, unseen data rather than memorizing the training set.
mzUe>y:1GK	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does linear regression model the relationship between variables?	Linear regression models the relationship between variables using a straight-line equation of the form y = mx + c, where m represents the slope (the rate of change) and c represents the intercept (the point where the line crosses the y-axis).
bXA0b9}08*	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why does regression use the least-squares approach?	Regression uses the least-squares approach to minimize the sum of squared differences between the observed values and the predicted values, ensuring that both positive and negative errors contribute equally and cancel less due to squaring.
z;On[90XPD	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the purpose of squaring errors instead of summing them directly?	Squaring errors prevents positive and negative errors from canceling each other out and gives more weight to larger deviations, allowing the regression line to better fit the overall trend.
KrOf7;Uxb`	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How are the coefficients m and c found in linear regression?	The coefficients m (slope) and c (intercept) are found by taking partial derivatives of the total squared error with respect to each coefficient, setting them to zero, and solving the resulting system of equations.
"Qj2#Sx[z?i"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the coefficient of determination (R²) represent?	The coefficient of determination, denoted R², measures how much of the variation in the dependent variable is explained by the regression model, with values closer to 1 indicating a better fit.
NzfW{oi6?N	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is considered a good R² value?	A good R² value is one that is close to 1, meaning the model explains most of the variance in the dependent variable; however, a higher R² does not always mean a better model if overfitting is present.
lt)cWU1~Al	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the standard error of regression?	The standard error of regression measures the average distance between observed and predicted values, showing how precisely the regression line fits the data; smaller standard errors indicate a better fit.
wi$tqjsOE4	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the F-statistic measure in regression analysis?	The F-statistic compares the model’s explained variance to its unexplained variance, helping to determine whether the regression model as a whole provides a better fit than a model with no predictors.
rO5a%NUD==	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a p-value in the context of regression?	A p-value tests the statistical significance of the regression coefficients by measuring the probability that the observed relationship could occur by random chance; a small p-value (typically below 0.05) indicates that the coefficient is statistically significant.
b,TT&[d(hD	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the null hypothesis in linear regression significance testing?	The null hypothesis in regression states that all coefficients of independent variables are zero, meaning there is no significant relationship between the independent and dependent variables.
F<D%oI8(ua	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does it mean if the p-value is less than 0.05?	If the p-value is less than 0.05, we reject the null hypothesis, concluding that there is a statistically significant relationship between the independent and dependent variables.
sR2u1R%o,f	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is multidimensional linear regression?	Multidimensional linear regression, also known as multiple linear regression, models the relationship between several independent variables and a single dependent variable, fitting a hyperplane instead of a simple line.
ASvsO<h_1R	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does the equation of a multidimensional regression differ from simple regression?	In multidimensional regression, the model equation is y = b0 + b1x1 + b2x2 + ... + bnxn, where each coefficient represents the contribution of its respective feature to the dependent variable.
"w%JQYWL#OG"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is polynomial regression used for?	Polynomial regression is used when the relationship between the dependent and independent variable is nonlinear, fitting curves of higher degrees (quadratic, cubic, etc.) instead of a straight line.
lM,3^HIS@H	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What happens when higher-degree polynomials are used in regression?	Using higher-degree polynomials allows the model to fit complex, nonlinear patterns but increases the risk of overfitting, as the curve may begin fitting noise rather than meaningful trends.
B-HaR!y-s}	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do R² and F-statistic help in choosing between polynomial fits?	Higher R² and F-statistic values generally indicate better fits, but a simpler polynomial with a similar performance (e.g., quadratic vs. cubic) is often preferred for better generalization and interpretability.
"n#.d_zv*Fq"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is overfitting in regression?	Overfitting occurs when a regression model learns noise or random fluctuations in the training data rather than the true underlying pattern, resulting in poor generalization to new data.
w[0+XW:c^,	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is underfitting in regression?	Underfitting occurs when a model is too simple to capture the relationships within the data, leading to high error on both training and test datasets.
"v6#>Ve/!wX"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is generalization important in regression models?	Generalization ensures that a model performs well on unseen data by capturing underlying trends rather than memorizing specific examples from the training dataset.
O*mta:I&.f	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is regularization in regression?	Regularization is a technique that penalizes large coefficient values in the regression equation to prevent overfitting, leading to more stable and generalizable models.
B&YyZDteo!	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the parameter λ represent in regularization methods?	The parameter λ (lambda) controls the strength of the penalty applied to large coefficients; higher λ values increase shrinkage, reducing overfitting but potentially increasing bias.
nI~{:B4rg%	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is Ridge regression?	Ridge regression adds a penalty equal to the sum of the squares of the coefficients (L2 penalty) to the loss function, shrinking coefficients toward zero to stabilize the model without eliminating features.
CtqcM:eXU;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the main limitation of Ridge regression?	Ridge regression shrinks coefficients but never sets them exactly to zero, meaning all features remain in the model even if some have minimal influence.
L3U`Cd|YNE	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is Lasso regression?	Lasso regression adds a penalty based on the absolute values of the coefficients (L1 penalty), which can shrink some coefficients exactly to zero, performing both regularization and feature selection.
eT1l:58m&g	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When is Lasso regression especially useful?	Lasso regression is particularly useful when the dataset has many features and we want to identify the most important predictors by automatically reducing irrelevant coefficients to zero.
gs`)d9K(F1	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is Elastic-Net regression?	Elastic-Net regression combines both L1 (Lasso) and L2 (Ridge) penalties, balancing coefficient shrinkage and feature selection, and treating correlated predictors more evenly.
Fw3rM,{S!%	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do Ridge, Lasso, and Elastic-Net differ in treating correlated features?	Ridge regression shrinks correlated coefficients together, Lasso may keep one and drop others arbitrarily, and Elastic-Net balances both effects by combining their penalties.
Q5q5hue9*A	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What role does crossvalidation play in regularization?	Crossvalidation is used to determine the optimal value of the regularization parameter λ (and α for Elastic-Net) by testing performance on multiple data folds and selecting values that minimize error.
n)d:B.PqN3	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is k-Nearest-Neighbor (kNN) regression?	kNN regression predicts the dependent variable for a new example by averaging the target values of its k closest neighbors in the training set based on feature similarity.
"vVyR#1e1XC"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does Support-Vector Regression (SVR) differ from traditional regression?	Support-Vector Regression uses the concept of a margin of tolerance around the predicted line, fitting the data within a defined boundary and minimizing errors outside that margin.
GY.iG5ihf;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are decision tree and random forest regressors?	Decision tree regressors predict values by recursively splitting the data based on feature values, while random forests combine many decision trees to reduce variance and improve prediction stability.
I`(+2g~I3~	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is deep learning used for regression?	Deep learning performs regression using artificial neural networks that can model complex nonlinear relationships; large amounts of data are often needed for training, although pretrained networks can adapt to new tasks efficiently.
bmoLF-{l0)	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is few-shot learning in the context of regression?	Few-shot learning refers to using pretrained neural networks that can learn or adapt to new regression tasks using only a small number of task-specific examples.
nLe8~2Xn=w	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is zero-shot learning?	Zero-shot learning is the ability of a pretrained model to perform a regression (or other) task without any new examples of that specific task, leveraging generalized knowledge from prior training.
dpvat2Cs1M	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can overfitting be reduced besides regularization?	Overfitting can be reduced by collecting more data, using simpler models, applying early stopping during training, performing crossvalidation, or removing noisy features.
QK/)lF$*>=	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the key difference between linear and polynomial regression?	Linear regression fits a straight-line relationship, assuming constant change, while polynomial regression fits curved relationships where the rate of change varies across values of the independent variable.
CwZ*YW.Cx6	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the slope coefficient indicate in a regression model?	The slope coefficient indicates how much the dependent variable is expected to change for a one-unit increase in the independent variable, assuming all other variables remain constant.
x38`MFD.8r	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the intercept represent in a regression model?	The intercept represents the expected value of the dependent variable when all independent variables are zero.
"L`#jk}ogc>"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is statistical significance important in regression results?	Statistical significance ensures that observed relationships in the data are not due to random variation, giving confidence that the model’s predictors truly influence the outcome.
P58]1^slj.	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is classification in data science?	Classification in data science is a supervised learning task where a model learns to assign data examples to predefined categories or classes based on their features.
I!sV]kauqB	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the purpose of supervised learning in classification?	The purpose of supervised learning in classification is to train a model on labeled examples so that it can accurately predict the class of unseen examples.
cZw-X6br>^	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does classification differ from clustering?	Classification uses labeled data to assign known categories, while clustering is unsupervised and groups examples based on similarity without prior labels.
j`eO:[bG~y	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a class in classification problems?	A class is a predefined category that represents a group of objects sharing similar properties or features within a dataset.
GX@,R:I[p2	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a feature in machine learning?	A feature is a measurable property or attribute used to describe an example in a dataset, such as height, weight, or color.
H%Yh&z,~f0	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the two main types of machine learning algorithms?	The two main types of machine learning algorithms are supervised learning, which uses labeled data, and unsupervised learning, which operates on unlabeled data.
QbCa<Q{(]^	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a similarity or distance metric used for?	A similarity or distance metric quantifies how alike or different two data points are, forming the basis for algorithms such as k-nearest-neighbor and clustering.
uz]LH[~q_<	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the role of a model in classification?	A model in classification represents the learned mapping between features and class labels that can be used to predict classes for new data.
x)|VYegWDo	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are some common supervised learning algorithms?	Common supervised learning algorithms include nearest-neighbor, decision trees, support-vector machines, and neural networks.
OELE~IR~FE	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are ensemble classifiers?	Ensemble classifiers combine multiple classification models and make predictions through group decision methods such as voting or consensus to improve accuracy.
LzCJ<zv0Te	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a nearest-neighbor classifier?	A nearest-neighbor classifier predicts the class of a new example based on the classes of the most similar (nearest) labeled examples in the dataset.
fA;K((|b&t	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does the k-nearest-neighbor (k-NN) classifier work?	In k-NN, a new example is classified by examining the k closest training examples and assigning the majority class among them to the new example.
jwiy7ue/vz	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What happens when k = 1 in k-NN classification?	When k = 1, the classifier assigns the class of the single nearest training example to the new, unlabeled example.
nJ}xeIN@vc	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is the choice of k important in k-NN?	The choice of k affects model performance; small k values may cause overfitting, while large k values may oversmooth the decision boundary.
cws_f86Ane	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a K-D tree?	A K-D tree is a data structure used in nearest-neighbor search to efficiently organize and retrieve multidimensional data points for distance-based queries.
wKu=2.;Ci;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the most common distance measure used in k-NN?	The most common distance measure used in k-NN is Euclidean distance, which computes the straight-line distance between two points in feature space.
y/TY;P$Q*z	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is cosine similarity used for in k-NN?	Cosine similarity measures the cosine of the angle between two feature vectors, determining similarity based on orientation rather than magnitude.
jcZ_qAxJ*G	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is majority voting in classification?	Majority voting assigns the class that occurs most frequently among the k nearest neighbors to the new data example.
zC263Y-k;i	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is weighted voting in k-NN classification?	Weighted voting gives greater influence to closer neighbors by assigning weights inversely proportional to their distance from the example being classified.
fp2}&Py>r;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the inductive bias of k-NN classifiers?	The inductive bias of k-NN assumes that similar examples are likely to belong to the same class, effectively grouping examples by proximity in feature space.
NU-VJSIA*=	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a decision tree classifier?	A decision tree classifier is a model that represents decisions and their possible consequences using a tree-like structure of nodes and branches.
"sI4nqr`C#3"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does a decision tree classify data?	A decision tree classifies data by traversing from the root node through branches based on feature-based questions until reaching a leaf node that assigns a class.
k0c7bmZeRb	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the ID3 algorithm?	According to Chapter 6 of Fundamentals of Data Science, the ID3 algorithm is a decision-tree learning algorithm that selects features based on maximum information gain derived from entropy reduction.
s){!r3?gk7	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is entropy in decision trees?	Entropy measures the degree of randomness or impurity in a dataset; lower entropy indicates more homogeneous class distributions.
MmP3ZGeg!x	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When does a dataset have zero entropy?	A dataset has zero entropy when all its examples belong to the same class, indicating complete homogeneity.
B9$]^6+6bK	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does information gain represent in decision trees?	Information gain represents the reduction in entropy achieved by splitting the dataset based on a particular attribute, helping to choose the best feature for a decision node.
ca5Z$+:FYF	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the Gini index used for?	The Gini index is an alternative impurity measure used in decision trees to quantify how mixed the classes are in a dataset.
ey-(~p0qH*	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the goal when constructing a decision tree?	The goal is to build the most compact and accurate tree that classifies data correctly while minimizing overfitting.
tR*Zs&sdW,	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why can building the optimal decision tree be computationally expensive?	Building the optimal decision tree is expensive because the number of possible trees grows exponentially with the number of features and examples.
FlfrIOE[Q4	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is overfitting in decision trees?	Overfitting occurs when a decision tree becomes too complex and captures noise or random patterns in the training data rather than true general relationships.
dYYqao/Ps6	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why can a smaller decision tree sometimes perform better than a larger one?	A smaller tree may generalize better by avoiding fitting to random or accidental regularities present only in the training data.
OPXt^TpRr+	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can decision trees be converted to rule sets?	Decision trees can be converted into sets of if-then rules, where each path from the root to a leaf corresponds to one rule defining class membership.
g9L37JSGBI	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the advantages of converting a decision tree to rules?	Converting a decision tree to rules simplifies interpretation and allows independent pruning of conditions, improving transparency and sometimes performance.
M31sB!$fd:	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How are continuous attributes handled in decision trees?	Continuous attributes are discretized into intervals or ranges (e.g., low, medium, high) so that the decision-tree algorithm can handle them as categorical splits.
J~4XDqZobC	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is imputation in decision tree learning?	Imputation is the process of replacing missing attribute values in training examples with estimated values, often using the most common value or correlated attributes.
cO*ucY,a8-	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the inductive bias of decision-tree classifiers?	The inductive bias of decision trees favors shorter and simpler trees (Occam’s Razor), assuming simpler explanations are more likely to generalize correctly.
FX$]6I@.<(	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a support-vector machine (SVM)?	A support-vector machine is a supervised learning algorithm that finds the optimal separating hyperplane between classes by maximizing the margin between them.
~Aok{j!@2	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does a linear separator do in classification?	A linear separator divides examples of different classes using a line (in 2D) or a hyperplane (in higher dimensions) that separates the data points as cleanly as possible.
e=}XY+-grQ	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the maximum-margin principle in SVMs?	The maximum-margin principle seeks the hyperplane that maximizes the distance between the nearest points of different classes, improving generalization.
m&[N,:lBF1	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are support vectors in SVM?	Support vectors are the data points closest to the separating hyperplane that define the margin boundaries and are critical to determining the classifier.
y[7:,BiZ-|	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is the SVM optimization problem called quadratic programming?	The SVM optimization problem involves minimizing a quadratic objective function subject to linear constraints, making it a quadratic programming problem.
x*XmyFbytK	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the primal and dual formulation of SVM optimization?	The primal formulation expresses the original optimization problem directly, while the dual formulation transforms it to a form that can be computationally easier to solve.
O|AeQ/1d$W	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the kernel trick in SVMs?	The kernel trick transforms data into a higher-dimensional space where classes become linearly separable, enabling nonlinear classification using linear SVMs.
znUf6B|W3E	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why are SVMs considered powerful classifiers?	SVMs are powerful because they achieve high accuracy, work well with both small and large datasets, and can handle nonlinear data through kernel functions.
hDmltvF^I7	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can SVMs perform multiclass classification?	SVMs perform multiclass classification by combining multiple binary classifiers, commonly using the one-against-one or one-against-all strategies.
"AG`?F&#pva"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the one-against-one approach in SVMs?	In the one-against-one approach, an SVM trains a separate classifier for every pair of classes, and the class receiving the majority of votes is assigned to the example.
uE%k!+=1:7	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the typical accuracy of an SVM on the Iris dataset?	According to Chapter 6 of Fundamentals of Data Science, a linear SVM trained and tested on the Iris dataset achieves about 98% accuracy.
Pou5.O2:vi	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is incremental learning in machine learning?	Incremental learning is a process where a model updates its knowledge as new data arrives, without retraining from scratch on the entire dataset.
"C_%_G6#y{#"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is incremental learning useful?	Incremental learning is useful when data arrives sequentially or in real time, allowing models to adapt continuously to new information.
wZQI9{ZB/7	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an incremental decision tree?	An incremental decision tree updates its structure dynamically as new examples arrive, adjusting nodes and branches to reflect newly observed patterns.
e-I+$C!/^7	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the ID4 algorithm?	According to Chapter 6 of Fundamentals of Data Science, the ID4 algorithm builds decision trees incrementally by updating node statistics and recomputing information gain as new data arrives.
hnEdf^g^NG	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the ID5R algorithm?	The ID5R algorithm, developed by Utgoff, refines ID4 by restructuring the tree incrementally without discarding subtrees, preserving consistency with previously learned examples.
"JY3`H1>LE#"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the main difference between ID4 and ID5R algorithms?	ID4 rebuilds subtrees when a new attribute becomes more informative, while ID5R restructures the tree without discarding prior information, improving efficiency.
CY2W(VYly@	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What challenges arise in incremental learning?	Challenges in incremental learning include updating models efficiently, handling evolving data distributions, and managing limited memory or computation.
b5V.^oRe|r	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an artificial neural network (ANN)?	An artificial neural network (ANN) is a computational model composed of interconnected nodes (“neurons”) arranged in layers that transform inputs to outputs via weighted sums and nonlinear activation functions; ANNs learn the weights from data to approximate functions for tasks like classification, regression, and generation.
Q4bx=C+iVz	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does a biological neuron inspire ANN design?	A biological neuron receives signals via dendrites, integrates them in the soma, and emits spikes down the axon if a threshold is exceeded; similarly, an artificial neuron aggregates weighted inputs, adds a bias, and applies an activation function to produce an output.
"yC#]B?Yok."	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the typical layers in an ANN?	Typical layers are the input layer (features), one or more hidden layers (learned internal representations), and an output layer (predictions such as probabilities for classes or numeric values for regression).
h<go%,E%dr	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What role do activation functions play in neural networks?	Activation functions introduce nonlinearity so networks can model complex, nonlinear relationships; without them, stacked linear layers collapse to a single linear transformation and cannot capture nonlinear patterns.
CZjeT{7[s*	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the McCulloch–Pitts neuron?	The McCulloch–Pitts neuron is an early binary threshold unit that sums binary inputs and outputs 1 if the sum exceeds a threshold θ, otherwise 0; it uses fixed unit weights and has no learning mechanism.
Gn95I=-:5T	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What limitation does the McCulloch–Pitts model have?	It lacks learnable weights and cannot adapt from data, and its binary step rule restricts it to linearly separable logic, limiting applicability to real-valued, nonlinearly separable problems.
HXt+3EOq[1	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a perceptron?	A perceptron is a learnable linear classifier that computes y = sign(w · x + b), where w are weights, x inputs, b bias; it predicts +1 or −1 depending on the sign of the weighted sum.
CJiRp:@+8t	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is the bias term important in a perceptron?	The bias shifts the decision boundary, enabling the classifier to separate data not centered at the origin; mathematically, it augments the affine function w · x + b.
C|!}39G%$m	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does “linearly separable” mean?	Linearly separable means there exists a hyperplane w · x + b = 0 that perfectly separates the classes such that w · x + b > 0 for all positives and < 0 for all negatives.
t:v&RyKZd4	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does the perceptron learning rule update weights?	For a misclassified example (x, y) with y ∈ {+1, −1}, the update is w := w + η y x and b := b + η y, where η > 0 is the learning rate; correctly classified points produce no change.
ulk+b)xDYN	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the effect of the perceptron learning rate η?	A larger η takes bigger steps (faster but risk overshoot), a smaller η takes smaller steps (more stable but slower); η is typically small and tuned empirically.
KZQI2.ry[@	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why can a single-layer perceptron solve AND/OR but not XOR?	AND/OR are linearly separable in 2D input space, whereas XOR’s positives and negatives are interleaved so no single straight line separates them; therefore a single perceptron fails on XOR.
iV)9C9pRF7	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a multilayer perceptron (MLP)?	A multilayer perceptron is a feedforward network with one or more hidden layers of neurons; each neuron computes a weighted sum plus bias followed by a nonlinear activation, enabling modeling of nonlinear decision boundaries.
w.0{elwJEl	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why are MLPs called “dense” networks?	They are called dense (fully connected) because each neuron in one layer connects to every neuron in the next layer, leading to many parameters: if a layer has m units and the next has n, there are m × n weights plus n biases.
IZdkWSa}71	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is forward propagation?	Forward propagation computes outputs layer by layer: z^(ℓ) = W^(ℓ) a^(ℓ−1) + b^(ℓ), a^(ℓ) = φ(z^(ℓ)), proceeding from inputs a^(0) = x to outputs a^(L).
iIwZbVV-GD	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is backpropagation in simple terms?	Backpropagation computes gradients of the loss with respect to all weights by applying the chain rule backward from the output layer to earlier layers, enabling efficient gradient-based updates.
xkx)@>Cu1)	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why do we need a loss function during training?	A loss function quantifies the discrepancy between predictions and targets (e.g., error in classification or regression), guiding weight updates to reduce this discrepancy across the dataset.
"IEq:#y,#+-"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is mean squared error (MSE)?	MSE is the average of squared differences between targets y_i and predictions ŷ_i: MSE = (1/N) Σ_i (y_i − ŷ_i)^2; it is common for regression and in some autoencoder reconstructions.
oLN_qc)VT4	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is cross-entropy loss for binary classification?	For targets y ∈ {0,1} and predicted probability p = P(y=1|x), binary cross-entropy is −[ y ln p + (1 − y) ln(1 − p) ]; it penalizes confident wrong predictions heavily and aligns with probabilistic modeling.
I+l~Ph,u)-	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is categorical cross-entropy?	For multi-class targets with one-hot vector y and predicted distribution p over K classes, categorical cross-entropy is −Σ_k y_k ln p_k; with softmax outputs, it trains the network to place high probability on the correct class.
tB~:MV_Y$~	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is softmax and when is it used?	Softmax maps a vector z to probabilities p_k = exp(z_k) / Σ_j exp(z_j); it is used in the final layer for multi-class classification so outputs form a valid probability distribution.
IB4NZo5$AM	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is stochastic gradient descent (SGD)?	SGD is an iterative optimizer that updates parameters using gradients computed on a single example or a small mini-batch: θ := θ − α ∇*θ L_batch; it is computationally efficient and introduces helpful noise for escaping shallow minima.
sDry}f+R5I	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does “epoch” mean in ANN training?	An epoch is one full pass through the entire training dataset (possibly via many mini-batches); many epochs are typically required until validation loss stops improving.
FYBF$J;H!{	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why use mini-batches instead of single examples?	Mini-batches (e.g., 32–256 examples) provide more stable gradient estimates than single examples while being more efficient than full-batch computation, often improving convergence speed and generalization.
Cc/7aEi18S	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are common activation functions and their ranges?	Sigmoid: σ(z) = 1/(1+e^(−z)) with output (0,1). Tanh: tanh(z) with output (−1,1). ReLU: max(0,z) with output [0,∞). LeakyReLU: max(αz, z) allowing small negative slope.
i+D(&nRx5L	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is ReLU popular in deep networks?	ReLU is simple, alleviates vanishing gradients for positive activations, is computationally cheap, and induces sparse activations, often accelerating training and improving performance.
kP`S?Fr((r	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the vanishing gradient problem in deep nets?	In deep stacks with saturating activations (e.g., sigmoid/tanh), gradients can shrink exponentially when backpropagated, making early layers learn very slowly; ReLU-like activations and good initialization help mitigate this.
Hc;mc3hD=^	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a convolutional neural network (CNN)?	A CNN is a neural architecture that uses convolutional layers with shared-weight filters (kernels) scanning local receptive fields to extract spatial features efficiently, typically interleaved with nonlinearities and pooling, followed by small fully connected heads.
r`L^:7c9Mt	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do convolutional layers reduce parameters versus dense layers?	A 3×3 filter over a single-channel image has 9 weights regardless of input size, and filters are reused (shared) across spatial positions; in contrast, a dense layer connects every input pixel to every neuron, exploding parameters with input dimensions.
v4VZZ3}?Q}	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a receptive field in CNNs?	A receptive field is the local spatial region of the input that influences a specific activation in a higher layer; filters slide across the input to cover all positions with shared weights.
b:|F;0:f3M	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is a 2D convolution computed?	For a k×k filter K applied to an input patch I of size k×k, the output is sum*{u,v} K_{u,v} * I_{u,v}; sliding (with stride) computes a feature map where each position is that sum at a different input location.
A:T6pHtvp6	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What do “stride” and “padding” mean in CNNs?	Stride is the step size of the filter as it moves across the input (larger stride downsamples); padding adds border values (often zeros) to control output size and preserve edges.
f~Mo`A1uJX	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are feature maps?	Feature maps are the outputs of convolutional filters across spatial positions; multiple filters produce multiple maps that capture different learned patterns (edges, textures, shapes).
w{Bs=`x+A*	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is pooling and why use it?	Pooling down-samples feature maps by aggregating over local regions (e.g., max or average), reducing spatial resolution, parameters, and sensitivity to small translations while emphasizing salient features.
isb<ZFkBo0	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why do CNNs typically end with fully connected layers?	After hierarchical feature extraction, fully connected layers integrate global information to perform tasks like classification; the final dense layers map the flattened high-level features to class scores or regression outputs.
PyS&LZ0WX|	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an encoder–decoder architecture?	It is a two-part network where an encoder transforms inputs into a compact latent representation (code) and a decoder reconstructs an output from that code; it underpins tasks like translation, super-resolution, and image denoising.
z^5}Vmw%QW	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an autoencoder?	An autoencoder is an encoder–decoder trained to reconstruct its input (often with a bottleneck), learning compressed representations useful for denoising, dimensionality reduction, or feature learning without labeled targets.
"P6(#,1Dv$>"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is an autoencoder trained for denoising?	The network receives a noisy input x_noisy and is trained to output a clean target x_clean by minimizing reconstruction loss (e.g., MSE), learning to remove noise while preserving structure.
KA&$Ayx+QV	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What limitation do standard autoencoders have for generation?	They are deterministic: given the same input and code, they produce the same output and do not model uncertainty; they also lack an explicit mechanism to sample diverse, realistic new examples.
p2`w;G+5!3	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a variational autoencoder (VAE) in plain terms?	A VAE is a probabilistic autoencoder that learns a distribution over latent variables z (with mean μ and standard deviation σ per dimension) and decodes samples z ~ N(μ, σ^2) to reconstruct or generate data, enabling diverse outputs.
O@`inZ<q?Z	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does a VAE represent the latent code?	Instead of a single vector, the encoder outputs parameters (μ, σ) for each latent dimension; at training and inference time, a latent sample is drawn as z = μ + σ ⊙ ε with ε ~ N(0, I) (the “reparameterization trick”).
N2R5H4|9g{	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is the reparameterization trick needed?	It makes sampling differentiable by expressing z as a deterministic function of (μ, σ) and random noise ε, allowing gradients to flow through μ and σ during backpropagation.
n[O&5ru:~-	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What losses does a VAE optimize?	A VAE minimizes total loss = reconstruction loss (e.g., MSE or cross-entropy between input x and reconstruction x̂) + KL divergence between the approximate posterior q(z|x) = N(μ, σ^2) and a prior p(z) = N(0, I).
h7a9e=ojv!	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the VAE KL divergence encourage?	It regularizes the latent space so encodings stay close to a standard normal distribution (centered, unit variance), promoting continuity and completeness: nearby z decode to similar outputs, and arbitrary z samples decode to plausible outputs.
"fIwN#15KUx"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does an encoder–decoder differ from a general CNN classifier?	A general CNN classifier maps inputs to labels via a feature extractor plus classifier head, while an encoder–decoder maps inputs to structured outputs (often of similar shape), requiring a decoder that upsamples or sequences outputs.
Rig&[|s|en	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are Transformers at a high level?	Transformers are sequence models based on self-attention mechanisms rather than recurrence or convolution; they encode relationships among all positions simultaneously, supporting long-range dependency modeling and parallel computation.
k=dI}z(fet	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What problem does self-attention solve?	Self-attention lets each token attend to (weigh) all other tokens when forming its representation, capturing context and dependencies across arbitrary distances without sequential recurrence.
p-T?C&?oMD	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is multi-head attention?	Multi-head attention runs several attention mechanisms (“heads”) in parallel, each learning different subspace relations; their outputs are concatenated and projected, enriching representational capacity.
Q&P{xsn[ET	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why do Transformers use positional encoding?	Because self-attention is permutation-invariant, positional encodings (added to token embeddings) inject order information so the model can distinguish positions in sequences.
M~h~||T{0N	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is masked multi-head attention in decoders?	It prevents a decoder token from attending to future positions by masking them, enabling autoregressive generation where each output depends only on previous outputs and encoder context.
"Q@.<nv#}]u"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does an encoder–decoder Transformer perform translation?	The encoder builds contextual representations of the source sentence; the decoder generates the target sentence token by token, using masked self-attention over generated tokens and cross-attention over encoder outputs.
KD&VA_.{(n	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why are Transformers widely adopted beyond NLP?	The attention mechanism generalizes to images, audio, and graphs, providing strong performance with scalable parallelism; adaptations like Vision Transformers and audio Transformers leverage similar principles.
r72V2wy.Lc	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the purpose of the output “softmax layer” in classifiers?	It converts final logits into probabilities summing to 1 across classes, enabling probabilistic interpretation and use of cross-entropy loss for training.
"Ns#yL@/WO7"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does batch training affect gradient updates?	Gradients are computed per example and averaged across the batch, then used to update parameters once per batch; this stabilizes updates and accelerates training compared to purely per-example updates.
s*vy>G0LSb	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the difference between training and inference?	Training optimizes parameters by minimizing loss using labeled (or structured) targets and backpropagation; inference uses the learned parameters to compute outputs for new inputs without updating weights.
fEny|a.iY)	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do hidden layers in deep networks build hierarchies?	Lower layers detect simple patterns (edges or n-grams), middle layers compose them into motifs (textures or phrases), and higher layers form abstract concepts (objects or semantics), enabling powerful representations.
bgS}3]N1+}	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is parameter efficiency important in CNNs?	Efficient parameterization reduces memory and computation, lowers overfitting risk on limited data, enables deeper models, and accelerates training and inference.
"M92!L[#^4U"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the difference between max pooling and average pooling?	Max pooling selects the largest activation in a region (emphasizing the most salient feature), while average pooling computes the mean (smoother aggregation that may capture general presence).
nRua-YyYM~	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an output “logit”?	A logit is the raw, unnormalized score produced by the final linear layer before softmax or sigmoid; logits can be mapped to probabilities via softmax (multi-class) or sigmoid (binary/multi-label).
"evb`#qIiu/"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the gradient of the sigmoid function?	If σ(z) = 1/(1+e^(−z)), then σ′(z) = σ(z)(1 − σ(z)); this derivative is used in backprop to propagate gradients through sigmoid activations.
GBAKVY_1~:	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the derivative of tanh?	For tanh(z), the derivative is 1 − tanh^2(z); like sigmoid, it saturates near −1 and 1.
ntq[Bpg=sq	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the piecewise derivative of ReLU?	ReLU′(z) = 1 if z > 0 and 0 if z ≤ 0; the zero slope for negative inputs can cause “dead” neurons if inputs remain negative.
kP=1!hifrO	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why might leaky ReLU be chosen over ReLU?	Leaky ReLU sets a small slope α (e.g., 0.01) for z < 0 to avoid dead neurons and maintain gradient flow for negative activations.
Ac3E/y9m^A	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the role of bias terms in each layer?	Biases allow each neuron to shift its activation threshold independently of inputs, increasing representational flexibility, especially when inputs are zero-centered.
mC).U@2>j8	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you interpret cross-entropy numerically?	Lower cross-entropy implies predicted probabilities concentrate on correct classes; if the model assigns probability 1.0 to the true class, cross-entropy approaches 0; confident wrong predictions yield large loss.
gZS<PpjJj]	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why are autoencoders useful without labels?	They learn structure directly from data distributions by minimizing reconstruction error, enabling representation learning, denoising, anomaly detection, and pretraining for downstream tasks without manual labels.
i-2?KGYJB~	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does the encoder depth affect representation quality?	Deeper encoders can capture more abstract features but risk overfitting and vanishing gradients; careful architecture, normalization, and regularization manage this trade-off.
K@7QYa3QE5	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does “flatten” mean before dense layers in CNNs?	Flattening reshapes multi-dimensional feature maps (H×W×C) into a single vector so fully connected layers can operate on them.
Pa5LsRQ6_H	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are common pitfalls when training deep networks?	Pitfalls include poor learning rate selection, overfitting from insufficient data or regularization, vanishing/exploding gradients, data leakage, class imbalance, and inadequate evaluation splits.
"w#nd`&<@lO"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can you regularize ANNs?	Common techniques include weight decay (L2), dropout, data augmentation (especially for CNNs), early stopping, batch normalization, and appropriate model capacity control.
q4$^AOD%IG	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does dropout help?	Dropout randomly zeroes a fraction of activations during training, preventing co-adaptation, implicitly ensembling subnetworks, and improving generalization; at inference, all units are used with scaled activations.
B!56nMJ`0]	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is batch normalization (BatchNorm)?	BatchNorm normalizes activations within a mini-batch to have controlled mean and variance, then scales and shifts them with learnable parameters, stabilizing and accelerating training.
rw(L1}*O(Q	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do Transformers scale with sequence length?	Standard self-attention has O(n^2) time and memory with sequence length n; numerous variants (sparse, linear, clustered attention) are used to mitigate this for long sequences.
K}45sKdH`R	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does masking enable autoregressive generation?	A causal mask zeroes attention to future positions so each step’s prediction depends only on past tokens (and encoder context), enabling left-to-right generation of sequences.
c10o`6uVP,	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you evaluate classifier performance from network outputs?	Use metrics like accuracy, precision/recall/F1, ROC-AUC, and confusion matrices on held-out data; probabilities can be calibrated for decision thresholds.
Hp~K-y*UDI	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the difference between training loss and validation loss?	Training loss measures fit on the training set; validation loss measures generalization to unseen data; a widening gap (training loss ↓, validation loss ↑) often indicates overfitting.
n-x5SK)C_*	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the “bottleneck” in encoder–decoder models?	The bottleneck is the compact latent representation (lower-dimensional code) that forces the model to compress salient information; its size is a key hyperparameter controlling capacity and generalization.
"mmhDr<Hyz#"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do CNNs process color images versus grayscale?	For RGB images, filters have depth 3 and convolve across all channels simultaneously; grayscale uses single-channel filters; deeper feature maps in higher layers act like multi-channel inputs for subsequent convolutions.
tbh&,o7*3,	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an example of reconstruction loss in images?	Pixelwise mean squared error: L_rec = (1/N) Σ_p (x_p − x̂_p)^2, where p indexes pixels; alternatives include binary cross-entropy for normalized pixels or perceptual losses using feature distances.
lKwZe-7999	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does shared weight structure aid translation invariance?	The same filter detects a pattern regardless of where it appears; combined with pooling or stride, this yields robustness to small shifts in the input.
j[io/5<AU2	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is citing the book unnecessary for basic definitions?	Common definitions (e.g., what a perceptron is, what MSE or ReLU are) are widespread knowledge in machine learning; citing the book is reserved for chapter-specific setups or emphases rather than general definitions.
ixcAKpW;8A	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What distinguishes dense networks from convolutional layers computationally?	Dense layers perform matrix multiplications with global connectivity (O(N×M) parameters), while convolutional layers perform local, shared-weight operations (O(k×k×C×F) parameters), dramatically reducing parameter count for high-dimensional inputs.
t4hiz-!4Tk	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you compute a single convolution output numerically?	Example: with filter K = [[1,0,−1],[1,0,−1],[1,0,−1]] and 3×3 input patch I, output = Σ_{u,v} K_{u,v} I_{u,v}; this particular filter acts like a vertical edge detector.
"C<R~hKL#3f"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the function of the final classifier head in CNNs?	It maps flattened high-level features to logits via one or more linear layers (often with dropout/batchnorm), followed by softmax or sigmoid to produce task-specific predictions.
JGbo@t7Dg=	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does a VAE generate novel samples after training?	Sample z ~ N(0, I) from the prior and pass it through the decoder to obtain x̂; or encode an existing x to (μ, σ), sample z = μ + σ ⊙ ε, and decode for a stochastic variant of x.
wkVbUG+HER	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does the KL term affect VAE reconstructions?	A stronger KL term (relative to reconstruction) pushes z toward N(0,I), improving latent structure but risking blurrier reconstructions; weaker KL term improves per-sample fidelity but may yield overfitting or latent collapse.
K|dq>HGNEf	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why might one choose average pooling over max pooling?	Average pooling can better preserve overall feature presence and is sometimes preferred in global average pooling before classification to reduce parameters and improve interpretability.
i4JxQ~Ph+@	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does “global average pooling” do?	It averages each feature map over all spatial positions to produce one value per map, removing the need for large fully connected layers and reducing parameters.
qQ-8JW;>K0	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do positional encodings typically look?	Commonly sinusoidal: for position pos and dimension i, PE(pos, 2i) = sin(pos/10000^(2i/d)), PE(pos, 2i+1) = cos(pos/10000^(2i/d)); learned positional embeddings are also used.
g^34vm5gL=	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does an attention weight get computed in Transformers?	Scaled dot-product attention uses queries Q, keys K, values V: Attention(Q,K,V) = softmax( (Q K^T) / sqrt(d_k) ) V, applied per head with learned projections.
k!;}UiNoIr	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the decoder’s cross-attention do?	It queries the encoder’s outputs so each generated token can attend to the most relevant encoded source positions, aligning output with input context.
qvM_B]iUK|	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you decide when to stop training?	Use early stopping based on validation loss or accuracy, patience windows, and monitor for overfitting; also track learning curves and consider learning-rate scheduling.
bB(:Ti%b@u	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the practical benefit of mini-batch size tuning?	Batch size trades gradient noise and hardware efficiency: small batches improve generalization and allow more updates per epoch; larger batches are faster on GPUs but may require learning-rate scaling.
qeh*BXUn<Z	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How are weights typically initialized in deep nets?	Common schemes include Xavier/Glorot (for tanh/sigmoid) and He initialization (for ReLU) to keep activation variances stable across layers at the start of training.
v{wD/Qxx^+	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does “feedforward” mean in MLPs?	It means information flows strictly from inputs to outputs without cycles; there is no feedback of outputs to earlier layers during inference (training uses backprop but inference is acyclic).
GvR%28DQ<[	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you express the perceptron decision boundary?	The decision boundary is the hyperplane H = { x | w · x + b = 0 }; points with w · x + b > 0 are classified as +1 and < 0 as −1.
q[({;e:%.t	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a confusion matrix and why is it useful?	A confusion matrix tabulates counts of true vs. predicted classes, revealing per-class performance, class imbalance effects, and specific error patterns beyond overall accuracy.
L^h28UqKpM	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you interpret attention qualitatively?	Attention weights can be visualized to show which input tokens or regions the model focused on when producing an output, aiding interpretability and debugging.
f4[c$1(nwR	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are typical uses of encoder–decoder beyond images?	Sequence-to-sequence tasks like machine translation, summarization, speech recognition, and code generation rely on encoder–decoder frameworks (often with Transformers).
}.(1NFD[P	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do CNNs and Transformers differ for vision tasks?	CNNs rely on local inductive biases (translation equivariance, locality) and parameter sharing; Vision Transformers learn global relationships via self-attention but often need more data or augmentation; hybrids combine strengths of both.
"p#bZa8F7`m"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the relationship between logits and cross-entropy gradients?	For softmax with cross-entropy and one-hot target y, the gradient w.r.t. logits is (p − y), which simplifies backprop and is numerically stable when implemented with log-softmax.
p.[{{}C2cn	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you compute mini-batch gradient descent update?	Given batch B, parameter θ, learning rate α: θ := θ − α (1/|B|) Σ_{(x,y)∈B} ∇_θ L(f_θ(x), y); momentum/Adam variants add velocity/averaging terms.
&*<eBs(+c	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why might you choose tanh over sigmoid in hidden layers?	Tanh is zero-centered (outputs in −1 to 1), leading to more balanced gradients and often faster convergence than sigmoid’s strictly positive outputs.
B>,)_(T=Q3	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are common output activations by task?	Regression: linear output. Binary classification: sigmoid. Multi-class classification (exclusive): softmax. Multi-label classification: independent sigmoids per label.
IjK1(~V^=)	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can pooling harm performance if misused?	Excessive pooling can remove spatial detail needed for fine-grained tasks; modern architectures may reduce or replace pooling with strided convolutions or attention to retain information.
Pq.rCGY1?9	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the difference between training a VAE and using it?	Training optimizes encoder and decoder jointly with reconstruction + KL losses; using it involves sampling z (from prior or posterior) and decoding to generate or reconstruct outputs without gradient updates.
O*au!&@Uc?	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the difference between feature selection and feature extraction?	Feature selection chooses a subset of the original features (keep some, drop others), while feature extraction creates new features by transforming or combining the originals (project into a new space). According to Chapter 8 of Fundamentals of Data Science, selection targets relevance and nonredundancy among the existing features, whereas extraction reduces dimensionality by constructing m new features from n originals, often via projections.
LJ=PaM6{c1	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why do we reduce dimensionality in machine learning?	We reduce dimensionality to improve model accuracy and generalization, lower computational cost, mitigate the curse of dimensionality, and make models easier to interpret. According to Chapter 8 of Fundamentals of Data Science, redundant or irrelevant features confuse learning and slow computation; selecting or extracting fewer, more informative features addresses these issues.
"w{/@tL#]D]"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is meant by the “curse of dimensionality”?	The curse of dimensionality refers to phenomena that arise when data has many features: distances become less informative, data becomes sparse, and learning requires exponentially more samples. According to Chapter 8 of Fundamentals of Data Science, feature selection/extraction combat this by reducing dimensionality so similarity and separability are easier to compute.
bLWo&M._eV	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the four high-level steps in a typical feature selection workflow?	A typical workflow is: (i) generate candidate feature subsets, (ii) evaluate the generated subsets using criteria, (iii) decide goodness of subsets based on the criteria, and (iv) validate the chosen subset on a downstream task. According to Chapter 8 of Fundamentals of Data Science, algorithms may shortcut some steps, but these four are the core.
qvO4(^A&N`	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How many subsets exist if you exhaustively search all feature subsets for n features?	There are 2^n unique subsets (including the empty set and the full set). According to Chapter 8 of Fundamentals of Data Science, exhaustive search guarantees the globally best subset but is impractical beyond small n due to exponential growth.
dN!)}8+kw=	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are forward, backward, and bidirectional selection?	Forward selection starts from an empty set and greedily adds features; backward selection starts from all features and removes the least useful; bidirectional mixes both, alternately adding and removing. According to Chapter 8 of Fundamentals of Data Science, these are heuristic searches that do not guarantee the optimal subset.
pq(Za1/>j8	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When might random subset generation be useful?	Random generation can quickly explore the space and, on average, find good subsets with less computation than systematic searches. According to Chapter 8 of Fundamentals of Data Science, stratified random sampling (e.g., by clustering into strata) can improve efficiency by focusing sampling within meaningful subgroups.
u,;8@)kUVS	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What distinguishes independent vs. dependent evaluation metrics for feature subsets?	Independent metrics assess subsets using only intrinsic properties of the features (e.g., redundancy), while dependent metrics assess how a subset performs on a downstream model or task. According to Chapter 8 of Fundamentals of Data Science, dependent metrics can be external (use prior knowledge) or internal (use cohesion/separability without labels).
"IQ~I(_#%nb"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are filter, wrapper, embedded, and hybrid feature-selection methods?	Filter methods rank/select features using statistics independent of any specific model; wrapper methods search subsets using a predictive model to score performance; embedded methods select features as part of model training (e.g., Lasso, trees); hybrid methods use a filter to prune and then a wrapper to refine. According to Chapter 8 of Fundamentals of Data Science, hybrids aim to retain wrapper accuracy at lower cost by prefiltering.
y+@aA-`dfA	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does a filter method typically work?	A filter computes a relevance or redundancy score (e.g., correlation, mutual information) for features or feature pairs, ranks them, and selects those passing a threshold or top-k. According to Chapter 8 of Fundamentals of Data Science, filters can also use metaheuristics like genetic algorithms, particle swarm, ant colony, or simulated annealing to search rankings.
mL&CXNR,6H	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is Pearson’s correlation coefficient and how is it used in feature selection?	Pearson’s r measures linear correlation between two quantitative vectors x and y: r = sum_i((x_i - mean_x)(y_i - mean_y)) / sqrt( sum_i(x_i - mean_x)^2 * sum_i(y_i - mean_y)^2 ). In selection, high |r| between a feature and the target indicates relevance; high |r| between two features indicates redundancy. According to Chapter 8 of Fundamentals of Data Science, pairwise r can populate an n×n correlation table to guide inclusion/exclusion.
HKZNw@F>@p	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can correlation drive a simple greedy selection algorithm?	One approach repeatedly adds the yet-unselected feature most correlated with the label and, at each step, excludes features most correlated (on average) with the selected set to minimize redundancy. According to Chapter 8 of Fundamentals of Data Science, this process maintains two working sets (selected and excluded) and iterates until no features remain to consider.
p=+Bj-L5o@	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is mutual information (discrete case) and why is it useful?	For discrete feature X and label Y: I(X;Y) = sum_j sum_k p(x_j, y_k) * log( p(x_j, y_k) / ( p(x_j) * p(y_k) ) ). It quantifies the dependency between X and Y beyond linear correlation and captures nonlinear relationships. According to Chapter 8 of Fundamentals of Data Science, mutual information supports both supervised (feature–label) and unsupervised (feature–feature) selection.
leA_)(*ajA	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does mutual information extend to continuous variables?	For continuous X and Y with densities p(x), p(y), and joint density p(x,y): I(X;Y) = ∬ p(x,y) * log( p(x,y) / ( p(x) * p(y) ) ) dx dy. According to Chapter 8 of Fundamentals of Data Science, practical use assumes or estimates densities (e.g., Gaussians or nonparametric estimators) and integrates/sums accordingly.
"v05GGWN5f#"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the key difference between correlation and mutual information?	Correlation measures linear association; mutual information measures general statistical dependence (linear or nonlinear). As a result, mutual information can identify relevant features missed by correlation when relationships are nonlinear. According to Chapter 8 of Fundamentals of Data Science, sophisticated methods may combine both to balance sensitivity and robustness.
f!u<`izJCg	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the wrapper approach to feature selection?	A wrapper uses a predictive model to evaluate each candidate subset by training/validating the model and using the resulting performance (e.g., accuracy, AUC) as the subset’s score. According to Chapter 8 of Fundamentals of Data Science, wrappers can achieve high accuracy but are computationally expensive; mRMR+wrapper and evolutionary wrappers are cited variants.
zB{b]!+*du	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the embedded approach and an example?	Embedded methods select features during model training via built-in regularization or structure. Examples include L1-regularized (Lasso) linear models that drive some weights to zero and tree-based models (e.g., Random Forests) that induce feature importance via splits. According to Chapter 8 of Fundamentals of Data Science, embedded selection improves efficiency but inherits model bias.
h?FeO&7W<H	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do hybrid methods combine filter and wrapper strengths?	A hybrid first applies a filter to remove obviously irrelevant/redundant features (reducing search space), then runs a wrapper over the survivors to refine the subset using model-based validation. According to Chapter 8 of Fundamentals of Data Science, hybrids reduce wrapper cost and often improve accuracy over pure filters.
jJYk9?|=Op	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is ensemble feature selection?	Ensemble feature selection aggregates rankings or subsets produced by diverse base selectors (e.g., different filters, wrappers, models) to form a more robust consensus ranking/subset. According to Chapter 8 of Fundamentals of Data Science, diversity among base selectors helps errors cancel out; a combination function aggregates rankings into a final list.
q$qJx8zJ?M	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When is incremental feature selection appropriate?	Incremental selection is used when data arrives over time and recomputing selection from scratch is costly. The algorithm updates the chosen subset as new instances arrive, avoiding full recomputation. According to Chapter 8 of Fundamentals of Data Science, incremental LVF variants and other probabilistic methods address dynamic datasets (e.g., gene expression, intrusion detection, text streams).
ggsld[PQhO	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does PCA differ from feature selection?	PCA produces new, uncorrelated features (principal components) that are linear combinations of the originals, ordered by variance explained; feature selection keeps a subset of original features. According to Chapter 8 of Fundamentals of Data Science, PCA targets variance capture and decorrelation rather than explicit relevance to labels.
dM)PU9m0I3	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are principal components in PCA?	Principal components are orthogonal directions in feature space along which the projected data has maximal variance (first PC), then maximal remaining variance under orthogonality (second PC), and so on. Each component is a linear combination of original features. According to Chapter 8 of Fundamentals of Data Science, PCs can be used to reduce dimensions while retaining most variance.
IL]=hm2o3$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is PCA computed in practice?	Common steps are: standardize features; compute the covariance matrix Σ of standardized data; find eigenvalues λ_i and eigenvectors v_i of Σ; sort eigenvectors by descending λ_i; choose top k eigenvectors; project data onto the matrix V_k to obtain k-dimensional representations. According to Chapter 8 of Fundamentals of Data Science, eigenvectors define component directions and eigenvalues quantify variance explained.
PGPhHX}$!^	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you decide how many principal components to keep?	Choose the smallest k such that cumulative variance explained (sum of top k eigenvalues / sum of all eigenvalues) meets a target like 90%, 95%, or 99%. According to Chapter 8 of Fundamentals of Data Science, real datasets often concentrate variance in a few leading components, enabling large reductions with small loss.
jL~k4y,7Nn	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the first principal component vs. a regression line?	A regression line (least squares) minimizes squared vertical errors predicting y from x; the first principal component maximizes total variance of projections (or equivalently minimizes orthogonal reconstruction error) without privileging any single axis. According to Chapter 8 of Fundamentals of Data Science, the PC line is the direction of maximal scatter, not a predictor of one variable from another.
Ri/z&dd.n&	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why are principal components orthogonal?	Orthogonality ensures each component captures new variance not explained by previous components, avoiding redundancy. Mathematically, covariance off-diagonals vanish in the PC basis, making components uncorrelated. According to Chapter 8 of Fundamentals of Data Science, each new PC is computed under the constraint of orthogonality to prior PCs.
eSuPICn|BB	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What happens to similar and dissimilar examples after PCA?	After projecting onto informative PCs and dropping low-variance components, similar examples often become more tightly clustered and dissimilar examples more separated because noise and redundant variation are suppressed. According to Chapter 8 of Fundamentals of Data Science, this typically improves downstream tasks like classification or clustering.
fdhzm9fy~*	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do pooling or ranking rules work in an ensemble selector?	Each base selector outputs either a ranking or a subset; an aggregation function (e.g., Borda count, rank-sum, median rank, majority vote on inclusion) combines them into a final ranking/subset. According to Chapter 8 of Fundamentals of Data Science, aggregation should reduce individual algorithm bias and leverage diversity.
hL2qh]Y<Rq	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What considerations guide choosing filter statistics for different data types?	Use correlation coefficients (Pearson, Spearman) for numeric–numeric relations; chi-square or mutual information for categorical; mixed-type strategies or mutual information for numeric–categorical pairs. According to Chapter 8 of Fundamentals of Data Science, the choice must match measurement scales and distributional assumptions.
hE$hkwwho>	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a correlation matrix and how is it used in selection?	A correlation matrix is an n×n table of pairwise correlations between features where diagonal entries are 1. High absolute off-diagonal values indicate redundancy; low values suggest complementary information. According to Chapter 8 of Fundamentals of Data Science, such a matrix guides dropping one of two highly correlated features.
lB|1?:CAoB	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why might embedded methods miss important features?	Because embedded selection reflects the inductive biases of the underlying model (e.g., linearity in Lasso, split criteria in trees). Features unhelpful to that model’s structure may be undervalued despite being useful to other models. According to Chapter 8 of Fundamentals of Data Science, this model bias can lead to nonselection of some important features.
vNo.M-hq$U	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can hybrid methods reduce wrapper complexity?	By first pruning via a cheap filter (e.g., keep top-k by MI) to shrink the search space, then running the expensive wrapper over far fewer candidates. According to Chapter 8 of Fundamentals of Data Science, this two-stage pipeline retains strong performance while curbing computational cost.
B31RY=7,?P	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What performance metrics validate a chosen feature subset?	Common metrics include accuracy, precision/recall/F1, AUC, log-loss for classification; RMSE/MAE/R^2 for regression; and clustering validity indices for unsupervised tasks. The metric should align with the downstream objective and be assessed via proper validation (e.g., cross-validation). According to Chapter 8 of Fundamentals of Data Science, validation is the final step.
N:I?%S,N]8	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can stratified random sampling aid random subset generation?	By partitioning the feature space into strata (e.g., via clustering) and sampling subsets within each stratum, the search explores diverse regions more systematically, improving the odds of finding strong subsets quickly. According to Chapter 8 of Fundamentals of Data Science, this balances randomness with coverage.
"eCSed^Fg#6"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What simple rule balances relevance and redundancy during greedy selection?	Select the next feature that maximizes: score = relevance(feature, target) − λ * average_redundancy(feature, selected_set), where relevance can be MI or |correlation| and redundancy is average pairwise dependence with selected features; λ ≥ 0 controls redundancy penalty. According to Chapter 8 of Fundamentals of Data Science, this captures the max-relevance/min-redundancy principle.
tJ2*?{UVug	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Write the discrete mutual information formula clearly in plain text.	I(X;Y) = sum_j sum_k p(x_j, y_k) * log( p(x_j, y_k) / ( p(x_j) * p(y_k) ) ). According to Chapter 8 of Fundamentals of Data Science, this sums over all joint outcomes and compares joint probability to the product of marginals.
lD$gCjVbVE	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Write the continuous mutual information formula clearly in plain text.	I(X;Y) = integral over x integral over y of p(x,y) * log( p(x,y) / ( p(x) * p(y) ) ) dy dx. According to Chapter 8 of Fundamentals of Data Science, densities replace probabilities and double integration spans the supports.
z{YKKC({LO	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Write the Pearson correlation coefficient clearly in plain text.	r(x,y) = sum_i ( (x_i - mean_x) * (y_i - mean_y) ) / sqrt( sum_i (x_i - mean_x)^2 * sum_i (y_i - mean_y)^2 ). According to Chapter 8 of Fundamentals of Data Science, values near 1 imply strong positive linear association, near -1 strong negative, near 0 weak.
m|aVx;Ua*g	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you compute the number of candidate subsets evaluated by exhaustive search vs. forward selection adding one at a time?	Exhaustive: 2^n. Greedy forward that examines every not-yet-selected feature at each of k additions evaluates roughly n + (n-1) + ... + (n-k+1) = k*(2n - k + 1)/2 subsets. According to Chapter 8 of Fundamentals of Data Science, greedy methods scale far better than exhaustive search.
bm^p3Xs$D	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why standardize features before PCA?	Standardization (zero mean, unit variance) prevents high-variance features from dominating the covariance matrix and thus the principal components. According to Chapter 8 of Fundamentals of Data Science, standardization ensures each original feature contributes comparably to PCA.
h-L-1~)G.O	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does PCA projection transform data points?	Given data matrix X (standardized) and V_k (columns are top k eigenvectors), the reduced representation is Z = X * V_k. Each row of Z is the k-dimensional embedding of an example. According to Chapter 8 of Fundamentals of Data Science, this projection preserves maximal variance along chosen directions.
AYC)r_*jO_	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does dropping low-variance PCs help downstream learning?	Low-variance components often capture noise or minor fluctuations; removing them compresses data, reduces overfitting risk, and speeds computation with minimal loss of signal. According to Chapter 8 of Fundamentals of Data Science, keeping only top PCs typically preserves most structure relevant to analysis.
ix(DKDQ<cM	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are examples of validation designs when comparing feature subsets?	Use nested cross-validation (inner loop selects features; outer loop estimates performance), or a holdout validation with strict separation of selection and testing. According to Chapter 8 of Fundamentals of Data Science, leakage must be avoided by performing selection only on training folds.
G*WX8s|f(K	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do ensemble and incremental selection complement each other in dynamic settings?	An ensemble can stabilize rankings across methods, while an incremental algorithm updates the chosen subset as data evolves; together they provide robustness to method bias and data drift. According to Chapter 8 of Fundamentals of Data Science, this pairing is valuable for streaming or time-varying domains.
x8=/45zN`~	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Summarize when to prefer selection vs. extraction in practice.	Prefer selection when model interpretability and original feature semantics matter, and when redundant/irrelevant features exist; prefer extraction (e.g., PCA) when decorrelation and compression of many correlated signals is key and interpretability of new axes is less critical. According to Chapter 8 of Fundamentals of Data Science, both aim to improve efficiency and performance but suit different constraints.
lg9ai{b^]$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is clustering?	Clustering is an unsupervised learning process that groups data instances so that items within the same group are highly similar while items from different groups are dissimilar. It discovers structure from data rather than relying on predefined labels.
tpzoBq8{*_	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does clustering differ from classification?	Classification assigns labeled data to pre-specified classes using supervised learning, while clustering finds groupings in unlabeled data based on a notion of proximity or similarity.
yjUQECWbL:	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is meant by “exclusive” clustering?	Exclusive clustering assigns each object to exactly one cluster. According to Chapter 9 of Fundamentals of Data Science, it can be formalized as a mapping f: D → {C1,...,Ck} where each object x ∈ D is mapped to one and only one cluster Cj.
u7g7WV;7ol	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an outlier in clustering?	An outlier (or anomaly) is an object that does not belong to any cluster because it does not share properties with cluster members. According to Chapter 9 of Fundamentals of Data Science, x is an outlier w.r.t. a clustering C if x ∉ ⋃ Cj.
FNF/(v%1<2	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is nonexclusive (overlapping) clustering?	Nonexclusive clustering allows an object to be a member of multiple clusters simultaneously, reflecting real scenarios (e.g., a user interested in both sports and films). According to Chapter 9 of Fundamentals of Data Science, the mapping can assign x to one or more clusters from C.
xls:pkSd71	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an intrinsic (embedded) community?	An intrinsic (embedded) community is a denser subcluster within a larger cluster. According to Chapter 9 of Fundamentals of Data Science, cluster A is embedded in B if A ⊂ B and A’s internal closeness is significantly higher than B’s.
JWw+fFais%	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why can multiple valid clusterings exist for the same data?	Different choices of attributes and proximity measures emphasize different notions of similarity, yielding different—but valid—groupings (e.g., grouping cards by color vs. shape).
Am?;Y4[k=;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why are proximity measures prerequisite for clustering?	They quantify similarity/distance between objects, which drives how clusters form; better measures yield more meaningful clusters.
w$5xZ-{2IG	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Define a proximity (similarity/distance) measure.	Given two vectors or distributions x and y, a proximity S returns a real score S(x,y) that reflects likeness (higher=more similar) or distance D(x,y) (lower=more similar). Typical properties: symmetry S(x,y)=S(y,x), positivity S(x,y)≥0, and reflexivity S(x,x) is maximal (or D(x,x)=0).
B+Y%]e*Z+S	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are standard distance/similarity measures?	Common choices include: (1) Euclidean distance: d(x,y)=sqrt(Σ_i (x_i−y_i)^2); (2) Manhattan distance: d(x,y)=Σ_i |x_i−y_i|; (3) Cosine similarity: cos(x,y)=(x·y)/(|x||y|); (4) Dot-product similarity: s(x,y)=x·y.
C1|TB)@+b.	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When is Euclidean vs. Manhattan distance preferred?	Euclidean emphasizes large coordinate differences and suits spherical clusters; Manhattan is more robust on grid-like or high-L1 structure and can be less sensitive to single large deviations along one axis.
"s5kJ`31]#2"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What statistical similarity measures are used?	(1) Pearson correlation r(x,y)=cov(x,y)/(σ_x σ_y) for linear relations; (2) Spearman rank correlation ρ measuring monotone association via rank; (3) Kendall’s tau τ based on concordant/discordant pairs. These focus on association rather than absolute distance.
q^7jhqf:p>	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are divergence measures between distributions?	They compare probability distributions: (1) KL divergence: KL(P||Q)=Σ_x P(x) log(P(x)/Q(x)) (asymmetric); (2) Jensen–Shannon divergence: JS(P,Q)=½ KL(P||M)+½ KL(Q||M) with M=(P+Q)/2, which is symmetric and bounded in [0,1] if base-2 logs are used.
Novv3bS^$=	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are kernel similarity measures?	A kernel k(x,y)=⟨φ(x),φ(y)⟩ computes dot products in an implicit feature space: (1) Linear: k(x,y)=x·y; (2) Polynomial: k(x,y)=(x·y + c)^d; (3) RBF: k(x,y)=exp(−||x−y||^2/(2σ^2)); (4) Laplacian: k(x,y)=exp(−||x−y||_1/σ); (5) Sigmoid: k(x,y)=tanh(γ x·y + C).
tlr9{/dr3H	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is partitional clustering?	It partitions data into k disjoint clusters by optimizing a cost (e.g., minimizing within-cluster variance), typically via iterative refinement. According to Chapter 9 of Fundamentals of Data Science, the goal is high intracluster similarity and low intercluster similarity.
gW!+08*x26	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the K-means objective?	Given clusters {C1,...,Ck} with centroids μ_j, minimize J=Σ_{j=1..k} Σ_{x∈Cj} ||x−μ_j||^2. The algorithm alternates assignment (nearest centroid) and update (recompute μ_j) until assignments stabilize.
w6Q?-~p3=X	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How fast is K-means and what are its limitations?	Runtime scales with data size, number of clusters, and iterations; in practice it often converges quickly. Limitations: assumes roughly spherical, equal-density clusters, is sensitive to initialization and outliers, and requires k a priori. According to Chapter 9 of Fundamentals of Data Science, it does not handle noise/outliers well and choosing k is nontrivial.
E9SI-zq3wb	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is PAM (K-medoids) and why use it?	PAM (Partitioning Around Medoids) replaces means with medoids (actual data points), minimizing Σ_{j} Σ_{x∈Cj} dist(x,medoid_j). Benefits: any distance metric can be used, and medoids improve interpretability and robustness to outliers.
o%Zpq:aYp=	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does PAM update medoids?	It iteratively considers swapping a current medoid m with a nonmedoid o, evaluates the change in total dissimilarity Δ(m,o), and performs the swap if it improves the objective, repeating until no improving swap exists.
qB,tlW%LCQ	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are CLARA and CLARANS?	CLARA runs PAM on samples to scale to large data, then assigns all points to the sampled medoids. CLARANS performs a randomized search over the medoid-swap neighborhood on a graph of solutions to find good local minima more efficiently.
"vEES$fQ#l3"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is hierarchical clustering?	It creates a nested set of clusters organized as a tree (dendrogram) using either agglomerative (bottom-up merges) or divisive (top-down splits) strategies, requiring a linkage rule to measure inter-cluster distance.
Je%ZE1=rP]	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are common linkage criteria?	Complete-linkage: dist(A,B)=max_{a∈A,b∈B} d(a,b); Single-linkage: dist(A,B)=min_{a∈A,b∈B} d(a,b); Average-linkage (UPGMA): dist(A,B)=avg_{a∈A,b∈B} d(a,b); Centroid-linkage: dist(A,B)=||μ_A−μ_B||.
NirlNogn2r	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you choose the number of hierarchical clusters?	Cut the dendrogram at a chosen level (height) and count the resulting connected components. According to Chapter 9 of Fundamentals of Data Science, a “cut line” at the maximal height avoiding merge points yields a practical choice.
Ck.5C;5=RR	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is BIRCH?	BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) incrementally builds a compact CF-tree (clustering feature tree) to summarize large datasets, enabling single-pass clustering with optional refinement. It is fast and scalable for dynamic streams.
zE[)ij{XK2	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is CHAMELEON?	CHAMELEON first forms many small subclusters by graph partitioning (k-NN graph), then agglomeratively merges subclusters based on relative interconnectivity and closeness, adapting to cluster shapes and densities. According to Chapter 9 of Fundamentals of Data Science, its overall complexity depends on graph construction and multilevel partitioning.
w{63?Xn81q	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is CACTUS?	CACTUS targets categorical data by summarizing attributes to binary vectors, clustering with a modified K-means using Jaccard distance, and then projecting frequent categories per attribute to describe each cluster. According to Chapter 9 of Fundamentals of Data Science, it operates as a subspace-style method for categorical domains.
qV6atj{ZNT	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is AMOEBA?	AMOEBA performs hierarchical spatial clustering using Delaunay triangulations: edges failing a proximity criterion are marked passive; active-edge components define clusters at each level as noise and passive edges are removed recursively.
yFoicCAD]t	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What motivates density-based clustering?	It can discover nonconvex clusters, handle noise explicitly, and infer the number of clusters from data density rather than requiring k upfront.
t8ACX6ak1/	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are epsilon (ε) and MinPts?	ε is the neighborhood radius; MinPts is the minimum number of points required within ε of a point for it to be a core. Together they define local density.
i+VHgpJVXK	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Define ε-neighborhood.	For point p, N_ε(p) = { q : d(p,q) ≤ ε }.
cnZrISfx/t	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Define a core point.	Point p is a core if |N_ε(p)| ≥ MinPts.
Dw-E!F_u)u	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Define “directly density-reachable.”	q is directly density-reachable from p if q ∈ N_ε(p) and p is a core point.
MM4;0;/G=4	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Define “density-reachable.”	q is density-reachable from p if there exists a chain p = x0, x1, …, xk = q where each x_{i+1} is directly density-reachable from x_i.
gn/m(n}oQD	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Define “density-connected.”	Points p and q are density-connected if there exists o such that both p and q are density-reachable from o.
pu@l<)0pCm	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a density-based cluster?	A (maximal) set of points all pairwise density-connected; border points are noncore points that fall within ε of some core point.
gU:)g_V4Z@	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is DBSCAN?	DBSCAN starts from an unvisited point: if it is a core point, it expands a cluster by iteratively adding all points density-reachable from it; otherwise it is labeled noise or a border. With a spatial index, runtime approaches O(n log n); without one, O(n^2). According to Chapter 9 of Fundamentals of Data Science, neighborhood queries dominate the cost.
npGa~RiBc}	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is OPTICS and its extra notions?	OPTICS orders points to reveal cluster structure across varying densities. It defines: core-distance of p as the smallest ε making p a core (distance to its MinPts-th nearest neighbor), and reachability-distance of q from p as max(core-distance(p), d(p,q)). The ordering plus reachability values allow extracting clusters at many density levels.
J[Zf$3q^?l	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does OPTICS differ from DBSCAN?	OPTICS does not output a single clustering; it produces an augmented ordering capturing multi-scale density structure, from which many DBSCAN-like clusterings can be extracted without re-scanning data.
rv4(`JU$t[	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What DBSCAN variants address limitations?	EnDBSCAN detects embedded/nested clusters and variable densities by extending core-distance to core-neighborhood ideas; GDBSCAN generalizes neighborhoods beyond ε and supports heterogeneous data; DENCLUE finds clusters via kernel density estimation and gradient ascent to density attractors.
i)0>Bdq|^_	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is clustering high-dimensional data hard?	In high dimensions, distances concentrate (many pairs are similarly far), making proximity less informative (“curse of dimensionality”). Clusters may exist only in subspaces, not globally.
Bb1[t[GM2Y	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How to handle high-dimensional data (preprocessing)?	Dimensionality reduction (e.g., PCA/SVD) creates fewer composite features capturing variance; feature selection picks informative subsets. These improve efficiency, though global transformations may miss local, cluster-specific correlations.
f~,qPnY-&o	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is projected clustering?	Projected clustering seeks clusters that are tight in selected subspaces rather than the full space. According to Chapter 9 of Fundamentals of Data Science, PROCULUS (projected K-medoids style) chooses medoids and, per cluster, identifies correlated dimensions (subspaces) for assignment and refinement.
hxOeA!R0rT	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is ORCLUS?	ORCLUS uses SVD/PCA-like analysis to find, for each cluster, directions (eigenvectors) of minimal variance and projects onto them to emphasize similarity; it hierarchically merges and lowers subspace dimensionalities to user targets.
dm+.5HU*%[	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is subspace clustering?	It simultaneously finds multiple clusters in potentially different subspaces, allowing objects to participate in different clusters across different attribute subsets (often called biclustering when rows and columns are clustered together).
fN/IRZwb:4	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does CLIQUE work?	CLIQUE grids each dimension into ξ equal intervals, marks dense units whose counts exceed τ, and performs bottom-up discovery of dense high-dimensional units by combining dense lower-dimensional ones; clusters are unions of connected dense units.
s%1<-T[nyC	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does MAFIA improve CLIQUE?	MAFIA adapts interval widths to data histograms and removes restrictive pruning, yielding higher-quality, faster subspace clustering with fewer rigid grid artifacts.
NRs4>K5a7+	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is CBF (Cell-Based Filtering)?	CBF builds fewer, data-driven bins (cells) using min/max-driven partitioning and a filtering index; bins above a density threshold are candidate cluster components, supporting large datasets with improved retrieval speed.
Q%55Df90K-	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a bicluster?	Given a matrix A (rows × columns), a bicluster is a submatrix defined by row set I and column set J whose entries satisfy a homogeneity criterion. According to Chapter 9 of Fundamentals of Data Science, biclustering is also called coclustering/block clustering.
L<NCC=J)l=	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What types of biclusters exist?	(1) Constant: a_ij ≈ θ (possibly with small noise); (2) Constant rows: each row i is constant across J (a_ij ≈ r_i); (3) Constant columns: each column j is constant across I (a_ij ≈ c_j); (4) Coherent additive: a_ij ≈ r_i + c_j; (5) Coherent multiplicative: a_ij ≈ r_i * c_j; (6) Coherent evolution (OPSM): rows share the same ordering pattern across columns.
b6jfy9fGE{	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is Mean Squared Residue (MSR)?	For submatrix (I,J), MSR = (1/(|I||J|)) Σ_{i∈I} Σ_{j∈J} (a_ij − a_iJ − a_Ij + a_IJ)^2, where a_iJ is row mean over J, a_Ij is column mean over I, and a_IJ is the bicluster mean. Lower MSR indicates stronger coherence.
iQR_S]X`l;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What did Cheng & Church contribute?	They introduced an MSR-based greedy biclustering that iteratively adds/removes rows/columns to find low-MSR biclusters, masking discovered biclusters and repeating to obtain multiple patterns.
B5cTX5I^5B	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why can MSR miss biologically relevant patterns?	MSR emphasizes absolute deviations and can miss shifting/scaling and inverse patterns where genes co-vary with different magnitudes or opposite directions.
e[V$)&n0:)	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does CoBi aim to capture?	CoBi (Coregulated Biclustering) models both positively and negatively regulated genes by tracking up-/down-regulation trends and fluctuation similarity, generating biclusters in polynomial time via a specialized BiClust tree. According to Chapter 9 of Fundamentals of Data Science, it handles inverted patterns that MSR-style methods can miss.
ihZ&JIrRhB	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why do we need cluster-validity measures?	To quantify the quality and stability of a clustering, compare algorithms/parameters, and avoid subjective interpretation—using external, internal, or relative criteria depending on available ground truth and goals.
dPT[Af&!pz	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is external evaluation?	It compares the discovered clustering to ground truth. Examples: (1) Rand Index RI=(TP+TN)/(TP+TN+FP+FN); (2) Fowlkes–Mallows FM=TP/sqrt((TP+FP)(TP+FN)); (3) Purity= (1/n) Σ_i max_j |C_i ∩ L_j|; (4) NMI = MI(p,r)/sqrt(H(p)H(r)), with MI(p,r)=Σ_{i,j} P(i,j) log(P(i,j)/(P(i)P(j))).
Iw,SH+JS!|	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is the Jaccard index used in clustering?	It measures set overlap: J(A,B)=|A∩B|/|A∪B|. According to Chapter 9 of Fundamentals of Data Science, a graph-based variant uses extended neighbor sets to compare local communities around nodes.
zq-=-?UGJ$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is internal evaluation?	It uses only the data and clustering structure: (1) Dunn Index DI = min_{i≠j} δ(C_i,C_j) / max_k Δ(C_k); (2) Silhouette s(i)=(b(i)−a(i))/max{a(i),b(i)} with a(i)=mean intra-cluster distance and b(i)=nearest-cluster mean distance; (3) Connectivity penalizes when an instance’s nearest neighbors fall into different clusters (lower is better).
kNDIx5t{3L	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is modularity Q?	Modularity compares observed intra-cluster edges to random expectation. General form: Q=(1/(2m)) Σ_{i,j} (A_{ij} − (k_i k_j)/(2m)) 1[c_i=c_j]. According to Chapter 9 of Fundamentals of Data Science (two-community case), Q = (1/(4m)) Σ_{i,j} (A_{ij} − (k_i k_j)/(2m)) s_i s_j with s_i∈{+1,−1}.
"M@ukTp<m[#"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is relative evaluation?	It compares multiple clusterings of the same dataset (e.g., different algorithms or parameters) using a chosen index to select the most suitable result when no ground truth is available.
pI`E{_u{?y	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How should one pick a clustering technique in practice?	Match method to data and goal: K-means/PAM for compact, roughly spherical clusters; hierarchical for multi-scale structure; density-based for irregular shapes with noise; projected/subspace/biclustering for high-dimensional or matrix data. Validate with internal indices and, when available, external labels.
M~C.b0J@(U	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do attributes and measures influence outcomes?	Choice of features and proximity determines which similarities drive grouping; standardize/normalize appropriately, select informative attributes, and use measures aligned with domain semantics (e.g., cosine for text, Jaccard for sets, divergences for distributions).
y1>Pa90m&W	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can we set ε and MinPts in DBSCAN?	Scan parameter grids guided by k-distance plots (distance to the MinPts-th neighbor) to find an elbow for ε; choose MinPts based on dimensionality (e.g., ≥ dimensionality + 1) and expected density—then validate via internal indices.
Hfk-&ajNYr	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are common pitfalls in clustering?	Ignoring scaling, mixing incomparable attributes without care, using a single metric for heterogeneous features, forcing k in unsuitable data, and over-interpreting clusters without validation or domain checks.
zp6MKpP3t}	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What postprocessing improves interpretability?	Label clusters using top features, prototypes/medoids, or descriptive rules; visualize via dendrograms, reachability plots (OPTICS), 2D embeddings (PCA/t-SNE/UMAP), and report stability across runs.
B2$`O5Kg7<	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What role do ensembles play in feature/cluster selection?	Combining diverse selectors/clusterers can mitigate individual biases and yield more stable rankings/partitions; aggregate via rank fusion or consensus clustering, then validate the consensus.
q1]KL<7hf9	Basic	Launch into Computing::Unit 05 - Data Science and Storage	In semisupervised scenarios, how is clustering helpful?	Use clustering to propagate a few labels within clusters (label spreading), initialize classifiers, or discover structure that guides feature engineering and sampling.
c!ONM;e<|0	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is ensemble learning? 	 Ensemble learning is the practice of combining multiple models (base learners) so their aggregated decision outperforms most single models on generalization, robustness, and stability.
"l#l^:_yukh"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why use an ensemble instead of a single model? 	 Because single models can overfit, underfit, or contain design/data biases; combining diverse learners reduces variance and compensates for individual weaknesses to improve accuracy.
PQz;,i0?nj	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does generalization mean in machine learning? 	 Generalization is a model’s ability to perform well on unseen data after learning from a training set by capturing essential patterns rather than incidental noise.
wsA*he1*|?	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is bias in a model? 	 Bias is the strength of assumptions a learner makes about the target function; high bias restricts the function class (e.g., only linear), often causing underfitting.
F:oh%eH+DN	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is variance in a model? 	 Variance measures how much a model’s predictions change with small changes in the training set; high-variance models (e.g., deep trees) tend to overfit.
tP|QGp26;&	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is overfitting? 	 Overfitting occurs when a model fits training data too closely and fails to generalize, performing poorly on new data.
hUz!d5D%oi	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How are ensembles categorized? 	 Ensembles commonly include supervised, unsupervised, semisupervised, metaensembles (ensembles of ensembles), and hybrid ensembles that combine supervised and unsupervised outputs.
L5^b!xrQ]F	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you construct different training sets for an ensemble? 	 You can resample examples with replacement (bootstrap), reweight examples across rounds, sample features (random subspaces), or vary architectures/hyperparameters across learners.
iX&u@ks0}r	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are base learners? 	 Base learners are the individual models whose outputs are combined; they can differ by algorithm family, hyperparameters, data subsamples, or feature subsets to encourage diversity.
F(:W<Ze+dh	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are combination learners? 	 Combination learners (or fusion rules) aggregate base-learner outputs—e.g., by voting on class labels or by combining continuous supports using sum/mean/product or weighted variants.
Gf2Q$[vN8F	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is majority voting? 	 Majority voting selects the class with the most votes among base learners; in case of a tie, predefined tie-breaking (e.g., class prior or highest-confidence voter) is used.
c11@L(;9__	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is weighted majority voting? 	 Each learner t has weight w_t; for class j we compute Score(j) = Σ_t w_t * I(h_t(x)=j), and predict the class with the largest Score(j).
j^pw9c<6<?	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you combine continuous outputs with the sum rule? 	 For class j, S_j(x) = Σ_c s_{c,j}(x), where s_{c,j}(x) is learner c’s support for class j; predict argmax_j S_j(x).
A,$ka3[4ce	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you combine continuous outputs with the mean rule? 	 For class j, S_j(x) = (1/C) * Σ_c s_{c,j}(x); this normalizes the sum by the number of learners C.
pRKgB==yA{	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does the weighted-sum rule work? 	 For class j, S_j(x) = Σ_c w_c * s_{c,j}(x), where w_c is the weight of learner c based on its reliability.
xK6p</UJNb	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does the product rule combine supports? 	 For class j, S_j(x) = Π_c s_{c,j}(x); this emphasizes consensus by penalizing any low support strongly.
KsWQ0TmTU]	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the max and min rules? 	 Max rule: S_j(x) = max_c s_{c,j}(x); Min rule: S_j(x) = min_c s_{c,j}(x); choose the class with the largest resulting S_j.
qMaLE7cIj<	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the generalized mean rule? 	For class j, S_j(x) = ( (1/C) * Σ_c [s_{c,j}(x)]^n )^(1/n), with n>0 controlling the degree between mean and max fusion.
BTV]!M42^<	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is consensus clustering? 	 Consensus clustering aggregates multiple clustering solutions by measuring pairwise agreement with a consensus function (e.g., Jaccard, adjusted Rand, NMI) and deriving a final clustering with maximal agreement.
AV/PYd4g1I	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is a consensus matrix defined? 	 According to Chapter 10 of Fundamentals of Data Science, given several clustering solutions, a consensus matrix C stores pairwise consensus values between clustering solutions (diagonal 1.0), which can be reclustered (e.g., hierarchically) to extract a consensus.
wUV@$zhE&c	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is supervised ensemble learning? 	 Supervised ensemble learning trains multiple classifiers on labeled data and combines their outputs (e.g., by voting or probability fusion) to reduce error relative to single models.
";v}`B2f#8"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is unsupervised ensemble learning? 	 Unsupervised ensemble learning forms several clusterings of the same unlabeled data (varying algorithms, seeds, features, or projections) and integrates them into a single, more stable consensus clustering.
MFJ.g0]F,:	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is semisupervised ensemble learning? 	 Semisupervised ensemble learning leverages both labeled and unlabeled data by using limited labels plus pseudo-labeling or graph-based regularization to expand training and improve performance.
FvPH(.:Qbq	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a metaensemble? 	 A metaensemble is an ensemble of ensembles whose outputs are further combined to mitigate biases of any single ensemble and improve robustness.
FiUIQ0u,n?	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does bagging work? 	 Bagging (bootstrap aggregating) trains base learners on bootstrap-resampled datasets and combines their predictions (often by majority vote), reducing variance and improving stability.
jz9Jw3l^6+	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does boosting work? 	 Boosting trains learners sequentially, increasing weights on previously misclassified examples so later learners focus on hard cases; final predictions are weighted by learner accuracy, which reduces bias and variance.
O,LbthaVn6	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a typical AdaBoost update? 	 With weighted examples w_i^(t), learner h_t has error ε_t and weight α_t = 0.5 * ln((1-ε_t)/ε_t); update w_i^(t+1) ∝ w_i^(t) * exp(α_t * I(h_t(x_i)≠y_i)), then renormalize.
OjA$$IhXBY	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is stacked generalization (stacking)? 	 Stacking trains level‑1 base models on training folds and feeds their out‑of‑fold predictions as features to a level‑2 meta‑learner that learns how to combine them while avoiding leakage via cross‑validation.
eV{qJmt4*6	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the random subspace method? 	 The random subspace method trains base learners on randomly sampled feature subsets to promote diversity; predictions are fused at decision time (classification or regression) or via consensus (clustering).
N3gl^l)`uJ	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How are ensembles used at the feature level? 	 Feature‑level ensembles combine ranked feature lists or importance scores from multiple feature selectors (e.g., via Borda count or weighted aggregation) to produce a more reliable subset.
B0V$hMTv~Q	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How are ensembles used at the decision level? 	 Decision‑level ensembles fuse the outputs of trained models (labels, probabilities, or supports) using voting, averaging, or learned meta‑models.
kgP9/l@*;_	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the key steps in an ensemble-learning framework? 	 Data preparation (cleaning, normalization, feature selection), model selection (diverse base learners), ensemble generation (resampling/reweighting/subspacing), model combination (voting or support fusion, or consensus for clustering), evaluation, and deployment.
t]<8MmMXGN	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is diversity among base learners important? 	 Diverse learners make uncorrelated errors; combining them reduces the chance that all fail on the same patterns, improving overall generalization.
iW,`5-xA$q	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Must base learners be accurate individually? 	 Yes—each should perform at least better than chance; combining learners that are all worse than random typically does not yield a strong ensemble.
dz?^LLx2qI	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does bagging reduce variance? 	 By averaging over multiple high‑variance learners trained on different bootstrap samples, bagging cancels out idiosyncratic fluctuations while preserving signal.
vce@TaAm~b	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does boosting reduce bias? 	 By focusing successive weak learners on the residual errors of prior learners, boosting adds capacity in regions the current model underfits, reducing systematic bias.
Hj,Xz9qU%M	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When do product and min rules hurt performance? 	 When any learner outputs near‑zero support for the true class, product and min rules can collapse the fused support, so they suit calibrated, reliable supports and multiple agreeing learners.
D]NMux5xGJ	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When is the max rule attractive? 	 When at least one learner is highly reliable per class but others are noisy, the max rule can recover that learner’s confident decision without dilution.
D]r8^ZYohy	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you set weights for weighted voting? 	 Practical choices include cross‑validated accuracy, log‑odds from error rates, or optimization on a validation set; according to Chapter 10 of Fundamentals of Data Science, principled weight selection remains an open issue.
u@h?T)VKuo	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How are unsupervised ensembles generated and integrated? 	 According to Chapter 10 of Fundamentals of Data Science, generation produces multiple clusterings (via resampling, subspaces, different algorithms/parameters); integration fuses them—often by voting on cluster memberships or by consensus functions.
uW_UzC*YIX	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a consensus function? 	 A consensus function quantifies agreement between clustering solutions (e.g., Jaccard, adjusted Rand, NMI) so that an aggregate solution can be chosen to maximize overall agreement.
oPtn57DuE[	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can ensembles handle mixed outputs (labels and scores)? 	 Map labels to one‑hot supports and calibrate scores to probabilities, then fuse with weighted‑sum/mean or train a meta‑learner; Chapter 10 notes mixed‑output fusion is a challenge that needs further study.
m,[s4OX+Vs	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do ensembles cope with high-dimensional data? 	 Use feature selection, PCA, random subspaces, or learners with built‑in regularization; combining subspace‑specialized learners can yield robust performance across sparse signals.
DG+zs;r2fX	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can you promote diversity in practice? 	 Mix model families (trees, linear models, SVMs, neural nets), vary hyperparameters/seeds, train on different bootstraps, or sample different feature subsets or projections.
glD>NAd2MM	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What metrics evaluate supervised ensembles? 	 Accuracy, precision, recall, F1, ROC‑AUC, PR‑AUC, calibration metrics, and cost‑sensitive measures depending on the application.
Pa0^RRCu&H	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What metrics evaluate clustering ensembles? 	 Internal indices (Silhouette, Dunn, Davies–Bouldin), external indices if labels exist (ARI, NMI, Fowlkes–Mallows, Purity), and stability under resampling.
D[g}.F$EZ!	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you avoid leakage in stacking? 	 Generate base‑model predictions for the meta‑learner using out‑of‑fold predictions (or a held‑out set) so the meta‑features simulate test‑time behavior.
QmN}R>/q,4	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is majority vs plurality vs consensus voting? 	Majority requires >50% of votes, plurality picks the most votes even if ≤50%, and consensus is full agreement; ensembles typically use plurality with tie‑break rules.
"j}=F(xP3)#"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is hybrid ensemble learning? 	 A hybrid ensemble fuses outputs from both supervised and unsupervised ensembles to leverage label‑driven accuracy and structure‑driven discovery in one decision.
tuT1]u=b0p	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you combine feature‑level and decision‑level ensembling? 	 First aggregate feature rankings to select a robust subset, train diverse classifiers on that subset, then combine their outputs via voting or stacking for a two‑stage ensemble.
g/*?i1jRC7	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does Random Forest relate to these ideas? 	 Random Forests are bagging ensembles of decision trees with random feature selection at each split, combining resampling and random subspaces to reduce variance and correlation.
n6n~atR,E$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you handle class imbalance in ensembles? 	 Rebalance bootstraps (e.g., stratified sampling), use class‑weighted losses, threshold moving, or cost‑sensitive fusion (weighted voting by per‑class performance).
n`CRg=a9px	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are practical tie‑breakers in voting? 	 Use class priors, average calibrated probability as a secondary key, or favor the prediction of the most accurate learner when votes tie.
CZ-3d%VP9G	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does semisupervised ensembling use unlabeled data? 	 It pseudo‑labels high‑confidence unlabeled examples or constrains learners on a graph/manifold so unlabeled data shape the decision boundary while labeled data anchor classes.
CzO)~oYs6e	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a graph‑based semisupervised ensemble? 	 According to Chapter 10 of Fundamentals of Data Science, one approach builds neighborhood graphs in multiple feature subspaces, trains semisupervised linear classifiers on these graphs, and ensembles them with multiobjective subspace selection.
Ev3`A`,H4L	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is FIEL in semisupervised ensembling? 	 According to Chapter 10 of Fundamentals of Data Science, FIEL (Frequent itemset and ensemble Learning) constructs a negative feature set from frequent itemsets, labels unlabeled data, and ensembles Naive Bayes, SVM, and Random Forest with majority voting.
rV&${SYv/p	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can rough sets be used in semisupervised ensembles? 	 According to Chapter 10 of Fundamentals of Data Science, a rough‑set approximation can first estimate negative samples from positive+unlabeled data; then an ensemble (e.g., Naive Bayes, SVM, Rocchio) iteratively refines the decision boundary.
efpv1%N=Tb	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What open challenges remain for ensembles? 	 According to Chapter 10 of Fundamentals of Data Science: principled weighting in weighted voting, fair fusion when some learners internally reduce dimensionality, mixing categorical and numerical data, and fusing mixed label/score outputs.
Hl(DDFS&yc	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you deploy an ensemble? 	 After validation and calibration, persist the base models and fusion logic, monitor post‑deployment drift and calibration, and periodically update or rebalance the ensemble with fresh data.
t5vwWHt$`&	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you choose between bagging, boosting, and stacking? 	 Use bagging for high‑variance learners and noisy data, boosting when you need to reduce bias and can tolerate sequential training, and stacking when you have heterogeneous strong learners and enough data to train a meta‑learner without leakage.
LqaD1J9ya<	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do ensembles improve robustness to noise? 	 Aggregation dilutes spurious patterns captured by any single learner, so random errors or outliers are less likely to dominate the final decision.
H7AG]!,Dc~	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are risks of overly correlated base learners? 	 Highly correlated errors don’t cancel; the ensemble adds little beyond a single model, so prioritize diversity (different families/parameters/subspaces).
q,2*2L5tVt	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Can ensembles overfit? 	 Yes—especially with high‑capacity base learners or a meta‑learner trained on in‑fold predictions; use cross‑validation, regularization, and early stopping to control overfitting.
KD[sKh0H.d	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you evaluate an unsupervised ensemble without labels? 	 Rely on internal validity indices, stability under resampling, agreement across consensus functions, and downstream task performance (e.g., cluster‑aware classification).
vhwK{^m9q,	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the role of preprocessing in ensembles? 	 Cleaning, normalization, missing‑value handling, feature engineering/selection, and dimensionality reduction strongly impact both base learners and the quality of fusion.
weF]ok|`3&	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does weighted combination relate to calibration? 	 Weighted probability fusion assumes calibrated supports; if not calibrated, first calibrate (e.g., Platt scaling, isotonic regression) or learn weights on a validation set to compensate.
yy/{{BXwEE	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When are metaensembles helpful? 	 When different ensembles (e.g., bagging vs boosting vs subspace) excel on different regimes or data slices, a metaensemble can blend their strengths.
gp*VBAEANa	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a practical recipe for a strong supervised ensemble? 	 Select diverse families (e.g., gradient‑boosted trees, linear model, SVM, neural net), generate out‑of‑fold predictions for stacking, calibrate probabilities, and fuse with a regularized meta‑learner.
D>,S|@?,D(	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is market-basket analysis?	Market-basket analysis models which items tend to be purchased together in transactions; it discovers associations such as {bread, jam} → {teabags} and informs merchandising, promotions, and layout.
qtzXZ<^,7f	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an itemset?	An itemset is a set of items considered together; a k-itemset has exactly k items.
qke{a00PBb	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an association rule?	An association rule is an implication P → Q between two disjoint itemsets P and Q indicating that when P occurs in a transaction, Q tends to occur as well.
vi*qbY9)T>	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is support defined?	Support(P) is the fraction of transactions containing itemset P; equivalently, Support(P) = count(P)/|D|.
B(1P|QH+Zj	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is confidence defined?	Confidence(P → Q) is the conditional probability that Q appears given P, i.e., Confidence = Support(P ∪ Q)/Support(P).
q=&f$rEq&x	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What makes a rule “strong”?	A strong rule is one whose Support and Confidence meet user-specified minimum thresholds (minsup and minconf).
n|iClVv&.(	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is lift and how do I interpret it?	Lift(P → Q) = Support(P ∪ Q) / (Support(P) × Support(Q)); lift > 1 suggests positive dependence, lift = 1 indicates independence, lift < 1 suggests negative dependence.
c59txtHNiT	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are conviction and leverage?	Conviction(P → Q) = (1 − Support(Q)) / (1 − Confidence(P → Q)); leverage = Support(P ∪ Q) − Support(P)×Support(Q); both quantify deviation from independence.
CL,We9$a^a	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why do we split association mining into two subproblems?	Because finding frequent itemsets is computationally expensive, while rule generation from frequent itemsets is straightforward; separating them focuses effort where it’s needed most.
bv,@%oW@D}	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is frequent itemset mining hard?	With n items there are 2^n − 1 possible nonempty subsets; exhaustive counting is infeasible on realistic datasets, so pruning or compression is required.
pg;b?(_C};	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What key idea does Apriori use to prune candidates?	According to Chapter 11 of Fundamentals of Data Science, Apriori uses downward and upward closure: all subsets of a frequent itemset are frequent, and no superset of an infrequent itemset can be frequent.
"t5h!wV=]#I"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does Apriori proceed level-wise?	It finds frequent 1-itemsets, then generates k-item candidates by joining frequent (k−1)-itemsets and prunes any candidate whose (k−1)-subsets are not all frequent; it repeats until no new frequent sets appear.
eSTP1GC0FV	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How many passes over the database does Apriori need?	According to Chapter 11 of Fundamentals of Data Science, Apriori needs m passes if the largest frequent itemset has size m; each pass counts supports for all candidates at that size.
"t5l#IQtaT#"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What drives Apriori’s runtime in practice?	The number of database scans and the number of candidates, which grows sharply when minsup is low (more frequent itemsets) or item universes are large.
ma8*/b_<A$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is candidate generation in Apriori?	It’s the join-and-prune step that forms k-item candidates from frequent (k−1)-itemsets that share (k−2) items, then drops any candidate with an infrequent subset.
wj,C8BoM5}	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is generating all 2-item candidates costly?	Because no subset pruning is available yet—every pair must be considered and counted, giving O(n^2) candidates for n items.
JT`k`:*m16	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is FP-growth at a high level?	FP-growth avoids candidate generation by compressing the database into a Frequent Pattern (FP) tree and recursively mining conditional pattern bases to enumerate all frequent itemsets.
QHH2]d[g^C	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How many database scans does FP-growth require?	According to Chapter 11 of Fundamentals of Data Science, FP-growth needs just two scans: one to count frequent items and a second to build the FP-tree over transactions ordered by frequency.
G[OEEL}%`a	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are FP-growth’s practical bottlenecks?	Large item universes or very low minsup can yield huge, memory-heavy FP-trees; building and traversing many conditional trees may still be expensive.
g3K~-Q+ko|	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When should I prefer FP-growth over Apriori?	When minsup is moderate to low and the data are compressible (shared prefixes), FP-growth typically outperforms Apriori by avoiding massive candidate sets.
d0hv+_?N;d	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the AIS algorithm and why is it less used?	AIS was the first approach but generates many unnecessary candidates and requires many full database passes; later methods like Apriori/FP-growth improved efficiency.
NU,)7Iy)d!	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can we reduce scans or memory beyond Apriori/FP-growth?	Techniques include vertical layouts, inverted matrices, COFI-trees, and one-pass structures (e.g., OPAM) to cut candidate storage and I/O.
Q,NCEtk+ky	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are quantitative association rules (QARs)?	QARs extend association rules to numeric and categorical attributes using value ranges, e.g., (Age ∈ [30,39] ∧ Married = Yes) → (NumCars = 2).
IChLfJo8JU	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Can QARs express negative relationships?	Yes. QARs can include negations in antecedent and/or consequent, e.g., (¬Married ∧ Age ∈ [20,29]) → (NumCars = 0).
l2iR0|]XHM	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does partitioning support QARs?	According to Chapter 11 of Fundamentals of Data Science, partitioning discretizes quantitative attributes into nonoverlapping intervals and treats each (attribute,interval) as a Boolean item for mining.
D/B_SD^~x}	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What’s the downside of naive discretization for QARs?	Coarse intervals cause information loss, may lower confidence/support for true rules, and can explode the number of trivial or redundant rules.
DkWRr9!NXp	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do clustering-based QAR methods work?	They map records into multi‑dimensional space, find dense regions (possibly via grids or density-connectivity), and convert those regions into high-support, high-confidence intervals for rules.
JfI>]v5Ke;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What makes some QARs “trivial” or “redundant”?	Rules whose ranges cover almost all values (trivial) or whose information is subsumed by tighter ranges (redundant) add little insight and should be pruned.
qx/Wn3^ZWv	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the MIC (Mutual Information and Clique) framework?	According to Chapter 11 of Fundamentals of Data Science, MIC computes pairwise mutual information, builds an MI graph, keeps attribute cliques above a threshold, and mines only those cliques to curb combinatorial interval joins.
t<h0V;9POy	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do fuzzy and evolutionary QARs differ?	Fuzzy QARs use linguistic terms and membership functions for intervals; evolutionary QARs (e.g., genetic algorithms) optimize rule sets (including negative rules) without prior discretization but cost more computation.
t=v(*z4[Z+	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why can support–confidence be misleading?	High support/confidence can arise from base rates; a rule may look “strong” under support–confidence yet reflect statistical independence or spurious association.
k=aOw9|yYB	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is correlation mining in this context?	It seeks item pairs that are statistically associated (positively or negatively) using correlation measures (e.g., phi, chi-square, or rank correlations) rather than just support–confidence.
"d86#QdkT9s"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is the phi (ϕ) coefficient computed for binary items?	ϕ(X,Y) = [ Support(XY) − Support(X)×Support(Y) ] / sqrt( Support(X)(1−Support(X)) Support(Y)(1−Support(Y)) ).
E%ERq~h,yN	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the “all-strong-pairs” correlation query?	Given a threshold θ, return all item pairs (X,Y) with ϕ(X,Y) ≥ θ; a related “top‑k” query returns the k pairs with largest ϕ.
D:Rt4K<Bw4	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is setting a ϕ threshold hard?	Without distributional knowledge, θ that is too high yields few/no pairs; too low yields many pairs and heavy computation, requiring iterative exploration.
za9OKzb$CV	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When is Spearman’s ρ preferable?	For nonparametric or noisy binary/ordinal data, Spearman’s rank correlation is robust to outliers and monotonic (not strictly linear) relationships.
h_*)da]k$y	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do we adapt Spearman’s ρ to binary market-basket data?	According to Chapter 11 of Fundamentals of Data Science, you assign tied ranks to 0s and 1s by frequency, compute squared rank differences using counts of (0,0),(0,1),(1,0),(1,1), then plug into ρ = 1 − 6 Σd_i^2 / (N(N^2−1)).
P5tDXt5:X2	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why bother with correlation mining at all?	It filters out rules that are “strong” only due to prevalence, highlights truly associated item pairs (including negative associations), and complements support–confidence mining.
]@s[sITzc	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is Count Distribution (CD) parallel mining?	According to Chapter 11 of Fundamentals of Data Science, CD partitions the database across processors; each counts supports for the full candidate set on its local partition, exchanges counts (sum‑reduction), prunes, then generates next‑level candidates in parallel.
AzZ8NbbEOp	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are CD’s pros and cons?	Pros: simple, minimal communication (only counts). Cons: each node replicates the full candidate set and counts the same number of candidates, under‑utilizing aggregate memory.
Az^lB1F6*Q	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is Data Distribution (DD) parallel mining?	According to Chapter 11 of Fundamentals of Data Science, DD assigns different candidate subsets to processors and circulates database partitions so each node counts only its candidates on all data; nodes then exchange local frequent sets and proceed to the next pass.
ci^n{MkL1E	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are DD’s pros and cons?	Pros: higher candidate throughput per pass and better memory use. Cons: increased communication because partitions (or their summaries) are exchanged each pass.
Hp%zcrWn4z	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How is rule generation parallelized?	Once the global frequent itemsets F are known (via CD or DD), each processor takes a disjoint subset of F, enumerates nonempty antecedents, and prunes with minconf using downward closure (no database access needed).
g0IW^Y{YYK	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What practical steps reduce ARM runtime?	Increase minsup moderately, use vertical layouts (tidlists), compress with FP‑trees, push sampling or filtering, exploit bitsets, and apply parallel CD/DD depending on network and memory.
ri7T2Gh63I	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do we handle high‑dimensional item universes?	Use feature selection (e.g., MI screening), compress rare items, leverage FP-growth’s prefix sharing, or adopt MIC to focus on informative attribute cliques before interval joining.
A^*Q]]cnrW	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do we avoid trivial/redundant QARs?	Prefer minimal ranges that meet thresholds, enforce dominance checks, penalize overly wide intervals, and use density or MDL‑style criteria to balance coverage and specificity.
Avy(`;<6F{	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do negative QARs help analysts?	They surface inhibitory relations, e.g., (¬PromoA) → (¬BasketB), informing cannibalization, assortment gaps, or risk controls; they should be evaluated with symmetric measures (e.g., leverage) to avoid base‑rate traps.
g1JaxSzab<	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are common pitfalls in ARM on streams?	Concept drift, unbounded memory, delayed feedback; use sliding windows, lossy counting, or sketch‑based support estimates to keep updates sublinear.
g.fx+%7SfT	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does majority voting differ from consensus in clustering ensembles?	Voting aggregates class labels; consensus clustering aggregates multiple partitions using agreement measures (e.g., NMI/ARI) to form a stable partition—useful when ARM outputs partitions of transactions or items.
P?`oTvk5*u	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a simple end‑to‑end ARM workflow?	Clean and encode transactions → choose minsup/minconf (or correlation θ) → mine frequent itemsets (Apriori/FP‑growth) → generate/prune rules → rank by lift/leverage/conviction → validate on hold‑out or time‑sliced data → deploy actions and monitor.
K1k`>,xU{I	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do I pick minsup and minconf sensibly?	Start from business prevalence (e.g., items with ≥1% of baskets), sweep minsup on samples to gauge candidate growth, then tune minconf alongside lift/leverage to balance coverage and novelty.
k2P&TCIsan	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do association rules drive decisions?	They inform cross‑selling, shelf placement, coupons, bundles, anomaly flags (e.g., fraud combos), and targeted recommendations; evaluate impact via A/B tests, not just offline metrics.
NwV))/a90E	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What credit is noted for quantitative mining content?	According to Chapter 11 of Fundamentals of Data Science, the quantitative association mining discussion acknowledges dissertation work by Mr. Dhrubajit Adhikary.
"ssM#acR5:4"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the 5 V's of Big Data?	The five key characteristics are volume, velocity, variety, veracity, and value. These describe how large data is (volume), how fast it is produced and must be processed (velocity), how many forms it comes in (variety), how trustworthy it is (veracity), and whether it produces useful outcomes (value). According to Chapter 12 of Fundamentals of Data Science, these 5Vs jointly frame Big Data’s challenges and opportunities.
w_hXzgoCS(	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do structured, unstructured, and semistructured data differ?	Structured data fits fixed schemas (e.g., relational tables), unstructured data lacks predefined organization (e.g., free text, images, video), and semistructured data carries tags/markers without rigid schemas (e.g., JSON, XML). Processing complexity generally rises from structured → semistructured → unstructured.
t(CGYBNZhA	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is veracity crucial in Big Data projects?	Veracity—data quality and trustworthiness—determines whether downstream insights are reliable. Low-veracity data (e.g., missing values, mislabeled records, noisy sensors) can mislead models and decisions. According to Chapter 12 of Fundamentals of Data Science, quality must be maintained while scaling volume, velocity, and variety.
x;8u(&d@8l	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does value mean in Big Data analytics?	Value is the net benefit realized from collecting, storing, and analyzing data—e.g., improved decisions, cost savings, or new revenue. Data that is stored but not converted into actions has low value.
JgA{k*6q81	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the three perspectives used to frame Big Data problems?	Data, processing, and management. The data perspective covers size, speed, quality, sensitivity, structure, and diversity; processing covers algorithms, scalability, actionable knowledge, and timelines (offline vs real time); management covers infrastructure, resource optimization, storage, and database technologies. According to Chapter 12 of Fundamentals of Data Science, these perspectives help scope challenges end to end.
B|3V?;/%ss	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why don’t traditional single-node databases scale for Big Data?	They are constrained by single-machine CPU, memory, storage, and I/O throughput. As data grows to terabytes/petabytes with mixed types and high arrival rates, a single node becomes a bottleneck for ingest, query, and failover.
hGU~H@Vs5+	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What problem does Hadoop Distributed File System (HDFS) solve?	HDFS stores massive datasets across commodity clusters with replication for high availability and fault tolerance. It co-locates compute with data blocks to reduce network movement and supports sequential, high-throughput reads/writes common in analytics.
gEhgmC6Um1	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is YARN in Hadoop?	Yet Another Resource Negotiator (YARN) manages cluster resources and schedules jobs (containers) across nodes, enabling multiple processing frameworks (e.g., MapReduce, Spark) to share the cluster.
t1_%})!Iu	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the MapReduce programming model in simple terms?	It splits a job into mappers and reducers. Mappers transform input into key–value pairs; reducers aggregate values for the same key to produce final output. The system handles data partitioning, shuffling, and fault tolerance.
L^^8vZr<|v	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is classic MapReduce inefficient for iterative algorithms?	Each iteration spins up new map/reduce tasks, repeatedly reads the same data from disk, and writes intermediate results back to disk, causing heavy I/O and startup overhead. According to Chapter 12 of Fundamentals of Data Science, this disk I/O bottleneck limits algorithms like graph traversal and gradient descent.
m;jQx$bIC:	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does Spark overcome MapReduce’s limitations?	Spark uses in-memory computation with Resilient Distributed Datasets (RDDs) and caching to avoid repeated disk I/O, enabling much faster iterative and interactive workloads while still integrating with HDFS/YARN.
z8v@O_vM2$	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What distinguishes peer-to-peer (MPI) systems from Hadoop-like systems?	MPI focuses on message passing among long-lived processes for high-performance, iterative, tightly-coupled computations; it preserves process state and requires users to manage fault tolerance. Hadoop/Spark emphasize data-parallel analytics with built-in fault tolerance over commodity clusters and higher-level abstractions.
hN~}>s{!YJ	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When should MPI be preferred over MapReduce/Spark?	When you need tightly-coupled, iterative numerical workloads with predictable communication patterns (e.g., linear algebra, scientific simulations) and are operating on HPC hardware with low-latency interconnects, accepting the responsibility to handle failures yourself.
Ld+!dS%|t[	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a fault-tolerant graph processing architecture?	It models computation as a graph of dependent tasks operating over shared/global state, supports iterative updates, and provides mechanisms to recover from node failures by re-executing affected tasks. According to Chapter 12 of Fundamentals of Data Science, GraphLab exemplifies this approach, trading some efficiency for robust, dependency-aware iteration.
bLh0xgwURp	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How does a streaming-graph architecture differ from MapReduce and GraphLab?	It processes data in-memory as it arrives, emphasizes asynchronous execution without a global shared store, and aims for very high throughput/low latency on streaming inputs. It typically sacrifices built-in fault tolerance; node failures may require restarting pipelines.
N)()?(;7||	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are typical Big Data analytics techniques?	Common techniques include machine learning (classification, regression, anomaly detection), data mining (pattern discovery like market-basket analysis), predictive modeling (forecasts, risk scoring), and visualization (dashboards and exploratory plots) to make patterns and trends interpretable at scale.
gydN~^ii!x	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is scalability a first-class requirement in Big Data architectures?	Scalability ensures systems can handle growing data volumes, users, and workloads by adding hardware rather than redesigning software. Architectures like MapReduce, graph processing, and streaming are designed to scale horizontally across many nodes.
c3C/cuy<XR	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What forms of heterogeneity must Big Data systems handle?	Heterogeneity in data types (text, images, logs, graphs), schemas, arrival rates, and quality; plus heterogeneity in compute resources (CPUs, GPUs, memory sizes), and network/storage performance across nodes.
DW^n4=y(J:	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What role do GPUs play in Big Data analytics?	GPUs provide massive parallelism and high memory bandwidth, accelerating compute-heavy tasks (e.g., deep learning, matrix ops). Frameworks like CUDA enable data scientists to exploit GPUs for order-of-magnitude speedups over multicore CPUs on suitable workloads.
D]Q.4P?Qt=	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do multicore CPUs contribute to parallelism on a single machine?	Multicore CPUs run many threads concurrently, sharing memory and I/O on one motherboard. Multithreading enables intra-node parallelism, complementing cluster-level parallelism for hybrid speedups.
C}F!d($)&K	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What advantages do HPC clusters offer?	HPC clusters provide many cores, high-speed interconnects, and low-latency communication suited to tightly-coupled computations, often with robust hardware fault tolerance and optimized MPI stacks.
v24+1d+/q;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why can storage become the bottleneck in Big Data pipelines?	Even with fast processors and networks, disks (or networked storage) may not keep up with read/write demands, especially for iterative jobs that repeatedly touch large datasets. SSDs, memory caching, and storage-aware job design mitigate this.
O85+NmwQ.;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the main stages of a Big Data pipeline?	Ingest (collect/stream), store (HDFS/object stores/databases), process (batch/stream/interactive compute), analyze/model (ML, mining), and serve/visualize (dashboards, APIs). Governance (security, lineage, quality) spans all stages.
q*=,:n<%Hn	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do batch and stream processing differ?	Batch processes large static datasets with high throughput but higher latency; stream (or real-time) processing handles unbounded, fast-arriving events, emphasizing low latency and incremental computation.
z|Bhax3d?7	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the business case for Big Data analytics?	Turning raw data into actionable insights—personalized recommendations, operational optimization, fraud detection, predictive maintenance, and better customer experiences—drives revenue, lowers costs, and enables new products.
d=r[E!mHf5	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are common data-management challenges in Big Data?	Provisioning scalable storage/compute, managing schemas and evolving formats, ensuring data quality and lineage, securing sensitive data, and optimizing resource usage across teams and workloads.
ox*xwx,?!k	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is HBase and when is it useful?	HBase is a NoSQL column-family store on HDFS for sparse, large tables needing fast random read/write and horizontal scalability (e.g., time-series or key–value access), complementing HDFS’s sequential I/O strengths.
ttLmcu1~]{	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do Pig and Hive help developers?	Pig (dataflow scripts) and Hive (SQL-like queries) compile into MapReduce/Spark jobs, letting teams write concise transformations/queries without managing low-level distributed code.
kCmu?cy7JS	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is fault tolerance central to distributed analytics?	With many commodity nodes, failures are common. Systems must detect, reassign, and recompute lost work or replicate state so jobs complete correctly without manual intervention.
"|$(U[8T#b"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What specific MapReduce roles do master and workers play?	The master coordinates job setup, splits, scheduling, and tracking; workers (compute nodes) run map/reduce tasks, read local HDFS blocks, write intermediate outputs, and fetch/shuffle data for reducers.
y4m9gc~H&&	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why do complex data dependencies strain MapReduce?	When steps require fine-grained, iterative exchanges (e.g., graph algorithms where one node depends on many neighbors’ latest state), repeated disk snapshots and global shuffles make MapReduce inefficient compared with graph/streaming engines.
Raybt1$Y,*	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What numeric facts illustrate today’s data deluge?	Billions of daily records and platform activity at minute-level scale (e.g., millions of messages, calls, posts, streams per minute). According to Chapter 12 of Fundamentals of Data Science, 2.5 quintillion bytes per day and platform-level minute stats demonstrate the velocity and volume now typical.
QAY1<@RMP~	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are the three Big Data architectures summarized in the chapter?	MapReduce (disk-based, scalable, fault-tolerant; weak for iterations), fault-tolerant graph (dependency-aware, iterative, fault-tolerant; higher I/O), and streaming-graph (asynchronous, in-memory streaming; limited fault tolerance). According to Chapter 12 of Fundamentals of Data Science, each targets specific needs among scalability, iterations, dependencies, and heterogeneity.
wgv?T5O|-1	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the Message Passing Interface (MPI) good at?	Coordinating explicit point-to-point and collective communications among long-lived processes in clusters/HPC for iterative, tightly-coupled computations, with features like broadcast, barriers, and reductions—but no built-in fault tolerance.
EhCegYA+,e	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do active challenges like I/O bottlenecks impact real time?	High network speeds (e.g., 4G/5G) generate data faster than disks can write/read, so repeated disk access becomes the rate limiter; architectures increasingly favor in-memory processing and careful data locality to keep up.
ESQ^KInq5M	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is handling heterogeneous semistructured data difficult?	Rigid schemas break on fast-evolving, diverse formats (logs, JSON, images). Systems must ingest without strict schemas, support schema-on-read, and still enforce governance and quality at scale.
H^8k$Wz)iF	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do handhelds and IoT intensify Big Data?	They proliferate sensors and apps that emit continuous, high-dimensional data (telemetry, location, usage), increasing both the number of events and the number of features per event; pipelines must scale in width and rate.
k(I{.[Hi<n	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What kinds of bioinformatics problems need Big Data tooling?	Constructing gene coexpression/regulatory networks, mining large protein–protein interaction graphs, aligning massive sequence datasets, and performing pathway analyses—all require distributed storage/compute and often GPU acceleration.
T-4&d]^d{	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Name examples of bioinformatics tools covered.	Beeline (parallel microarray analytics), caCORRECT (quality control), OmniBiomarker (biomarker discovery), FastGCN (GPU-based gene network), UGET and WGCNA (coexpression analysis), NETRA (regulatory networks), ClusterONE/MCODE/NeMo (PPI complexes), PathBLAST (PPI alignment), BioPig/SeqPig/Crossbow/Bowtie/SoapSNP/Stormbow/CloVR/Rainbow (sequence analytics), and GO-Elite/PathVisio/directPA/Pathway Processor/Pathway-PDT/Pathview (pathway analysis). According to Chapter 12 of Fundamentals of Data Science, these illustrate domain-specific Big Data tooling.
Fkx]/]4/M	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What computer-vision tooling was highlighted?	OpenCV for real-time vision; PyTorch and GluonCV for DL-based vision; NVIDIA CUDA-X for GPU-accelerated ops; and ITK for medical image processing.
"J}$hEj}a#M"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What NLP tooling was highlighted?	spaCy, Fairseq, Gensim, Flair, HuggingFace Transformers, CoreNLP, and Voyant—covering tagging/NER, sequence modeling, topic modeling, modern transformer stacks, Java NLP pipelines, and web-based textual analysis.
"pMMW%5V/#R"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What network-security tools exemplify data-driven analytics?	NuPIC and Loom for anomaly detection; XPack for monitoring and alerting; Splunk for large-scale log analytics; and QRadar and Cisco Stealthwatch for malware/threat detection on live network flows.
FOPPdF9ng(	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How should one pick among MapReduce, graph, and streaming architectures?	Match to workload: batch ETL/reporting suits MapReduce; iterative dependency-heavy analytics (e.g., PageRank, belief propagation) benefit from graph engines; low-latency event processing and continuous ML scoring suit streaming systems.
L8CGY&!8sh	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What are best practices to reduce I/O bottlenecks?	Cache hot datasets in memory (Spark caching), use columnar formats and compression, push computation to data (data locality), batch writes, minimize shuffles, exploit SSDs, and design algorithms with fewer disk passes.
H<@uE.%FeP	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do SQL and NoSQL coexist in Big Data?	SQL excels at structured, relational analytics and joins; NoSQL stores (key–value, document, column-family) handle flexible schemas, high write rates, and horizontal scaling. Many stacks combine them (e.g., Hive/Spark SQL with HBase/Cassandra).
J;RZlX*bZ6	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does “actionable knowledge” mean in this context?	Insights that directly inform decisions or automated actions—like targeted marketing, fraud flags, or maintenance schedules—rather than purely descriptive metrics.
O3{BO;??jV	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What risks arise if governance is ignored?	Data leaks, regulatory violations, and untrusted analytics. Strong governance enforces access control, encryption, lineage, retention policies, and quality rules across the pipeline.
yu:dzfYH9B	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you decide between on-prem HPC and cloud Big Data stacks?	Consider workload patterns (steady vs bursty), data gravity/compliance, required accelerators (GPUs/TPUs), cost models (CapEx vs OpEx), and operational expertise. Hybrid approaches often win: on-prem for steady, sensitive workloads; cloud for spikes and experimentation.
"ySkpr=Ln<#"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is Python commonly chosen for practical Data Science?	Python balances readability with a huge ecosystem (NumPy for array math, Pandas for tabular data, scikit‑learn for modeling, Matplotlib for plotting), strong community support, and easy integration into notebooks, services, and pipelines.
Nd4p;}Eh[|	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What do NumPy, Pandas, scikit‑learn, and Matplotlib each do?	NumPy provides fast n‑dimensional arrays and vectorized math; Pandas adds DataFrame/Series for labeled, relational‑like tables; scikit‑learn implements classic ML algorithms, model selection, metrics, and pipelines; Matplotlib renders static plots for EDA and evaluation.
s2r%qrBB|4	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you sanity‑check a freshly loaded dataset?	Start with head(), tail(), info(), dtypes, shape, describe(), nunique(), isna().sum(), and a quick value_counts() of the target to confirm schema, types, size, missingness, and class balance before any modeling.
NowXv(T)mU	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an appropriate first step if the dataset has mixed numeric and categorical features?	Identify nonnumeric columns, then encode them (e.g., one‑hot for nominal, ordinal encoding for ordered categories) while keeping a record of mappings to ensure reproducibility and correct inverse transforms.
P*u3|eyu9*	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How should missing values be handled safely?	Quantify missingness, then impute with a strategy appropriate to the feature (median for skewed numeric, mean for roughly normal numeric, most_frequent or a sentinel category for categoricals); perform imputation inside a pipeline fitted only on training splits to avoid leakage.
kLyAb!aH@.	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What train/test split strategy avoids optimistic bias?	Use train_test_split with stratify on the target for classification to preserve class ratios; keep the test set untouched until final evaluation; optionally carve out a validation set or rely on cross‑validation for model selection.
q/1Lf:VLF@	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When should cross‑validation be preferred over a single validation split?	When datasets are modest in size or variance is a concern; k‑fold CV averages performance across folds, reducing variance from a single lucky/unlucky split and yielding more reliable model comparison.
H`^v,bBQ?,	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is feature scaling often required?	Algorithms based on distance, margins, or gradient descent (e.g., SVMs, logistic regression, neural nets, K‑means) assume comparable feature scales; standardization (z‑score) or min‑max scaling ensures no feature dominates purely due to units.
Ar~!C2k<Lg	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a safe way to combine preprocessing and modeling?	Use scikit‑learn Pipelines and ColumnTransformers so that fit/transform steps occur in the correct order and are cross‑validated together; this prevents data leakage and simplifies deployment.
oH5N?`w/OK	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is feature selection and why do it?	Feature selection ranks or chooses a subset of predictors that are most informative for the target; this can reduce overfitting, improve generalization and training speed, and make models easier to interpret.
m(}S(__$YD	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can you perform simple filter‑based feature selection?	Apply univariate tests (e.g., mutual_info_classif, chi‑square for nonnegative counts, ANOVA F‑test for roughly normal features) via SelectKBest or SelectPercentile, then feed selected columns to the estimator within a pipeline.
c|%@D6BO]f	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you check redundancy among selected features?	Compute a correlation matrix (Pearson/Spearman as appropriate) on the selected set; investigate pairs with |corr| close to 1 to consider dropping one, being mindful of domain context and model needs.
NrQi/}f]Vl	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why is class imbalance a concern and how can you mitigate it?	"Imbalance can bias models toward the majority class; mitigate via stratified splits, class_weight=""balanced"" (for LR/SVM), threshold tuning, resampling (e.g., RandomOver/Under), or specialized metrics (ROC‑AUC, PR‑AUC, F1 for minority class)."
x%O;V)(XiP	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What metrics should be reported for classification beyond accuracy?	Include precision, recall, F1‑score, ROC‑AUC, confusion matrix, and possibly PR‑AUC—choose metrics that reflect costs of false positives/negatives in the application.
z3-6ll/p2*	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you interpret a confusion matrix?	Rows typically represent true classes and columns predicted classes (or vice versa—state it); diagonal cells are correct predictions, off‑diagonals are errors; inspect per‑class precision/recall to see who is being confused with whom.
t.8VNH1i*c	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is logistic regression good for here?	It provides a strong linear baseline, fast training, well‑calibrated probabilities (with proper regularization), and interpretable coefficients that indicate the direction and relative strength of feature effects after scaling.
i@=f2%,9;[	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When would you choose a Support‑Vector Machine?	When the decision boundary may be nonlinear (RBF kernel) or margins matter; SVMs handle high‑dimensional spaces well and can be robust to outliers with appropriate C and gamma; scale features and tune hyperparameters via CV.
DxySL_F%x!	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is an ANN in this practical context?	A feedforward multilayer perceptron (e.g., scikit‑learn’s MLPClassifier) trained with backpropagation; useful when relationships are nonlinear; requires scaled inputs, careful regularization (alpha, early stopping), and tuning of hidden layers.
w&kG->EdM&	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How should you evaluate models fairly?	Lock down preprocessing in pipelines, use stratified k‑fold CV for model selection, tune hyperparameters with GridSearchCV/RandomizedSearchCV, and report test results only once—after all choices are fixed.
qx`^7$cJLa	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is ROC‑AUC and when might PR‑AUC be preferred?	ROC‑AUC measures ranking quality across thresholds (TPR vs FPR); PR‑AUC focuses on precision‑recall tradeoffs and is more informative under heavy class imbalance where negatives dominate.
H$MFYu2ASB	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can you calibrate predicted probabilities?	Use CalibratedClassifierCV with sigmoid or isotonic calibration on a held‑out set or via CV—especially important for SVMs and tree ensembles whose raw scores aren’t probabilities.
kGT]SN*iJ=	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you persist trained models?	Export the entire pipeline with joblib.dump and reload with joblib.load to retain preprocessing, feature selection, and the estimator in one artifact.
BFC9)D|~H	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is K‑means and when does it work well?	K‑means partitions data into k clusters by minimizing within‑cluster squared distances; it performs well when clusters are roughly spherical and of similar size/density, and features are standardized.
O-uIVk|sc-	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you choose k for K‑means?	Use the elbow method (plot inertia vs k and look for a bend), complement with silhouette score and domain knowledge; avoid over‑interpreting small wiggles in inertia.
v@/BgQg;6R	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is inertia in K‑means?	Inertia is the sum of squared distances of samples to their closest cluster center; it decreases as k increases; large marginal gains tail off near the “elbow”.
G4K!{@ZnV{	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What does the silhouette score indicate?	For each sample, silhouette = (b − a) / max(a, b), where a is mean intra‑cluster distance and b is mean nearest‑cluster distance; values near 1 mean well‑separated, near 0 mean overlapping, negative mean likely misassigned.
bIF!I/0oOH	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is agglomerative clustering and how do dendrograms help?	Agglomerative clustering starts with each point as its own cluster and iteratively merges the closest clusters based on a linkage criterion; dendrograms visualize the merge tree so you can choose a cut height corresponding to a cluster solution.
J~6Uvd$3fo	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Which linkage criteria are common in hierarchical clustering?	Single linkage (min distance; can chain), complete linkage (max distance; compact clusters), average linkage (UPGMA), and Ward’s method (minimizes increase in total within‑cluster variance; requires Euclidean distances).
zzZav+$xGn	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you validate clustering if ground‑truth labels exist?	Use external indices such as Adjusted Rand Index (ARI), Normalized Mutual Information (NMI), or Purity—computed between the clustering labels and the known labels—recognizing that clustering is unsupervised.
t5mawqXtGD	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why scale features before clustering?	Distance‑based methods are sensitive to units; unscaled features can dominate similarity; standardize or normalize so heterogeneous features contribute comparably to distance computations.
ogtpSF3N`a	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What plots are most helpful during EDA in this workflow?	Histograms/ECDFs for distributions, boxplots/violin plots for spread/outliers, scatter or pair plots for relationships, heatmaps for correlation, and bar charts for class frequencies.
O;69f,G2RE	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can you avoid data leakage in practice?	Encapsulate imputation, scaling, encoding, and selection inside a pipeline; fit only on training folds; never compute anything on the full dataset that affects the model selection process.
"fvYP_9bF#?"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What hyperparameters matter for LR, SVM, and MLP?	LR: C (inverse regularization), penalty (l2), solver; SVM (RBF): C and gamma; MLP: hidden_layer_sizes, activation, alpha (L2), learning_rate, max_iter, early_stopping, batch_size.
nwhPa<twv;	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you choose between GridSearchCV and RandomizedSearchCV?	Grid search is exhaustive over small, carefully chosen grids; randomized search samples the space efficiently and often finds good configurations faster—useful when ranges are wide or training is costly.
tWP/;$:wa4	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the recommended way to visualize classification performance?	Plot ROC curves (and PR curves under imbalance), confusion matrices as heatmaps, and bar charts of precision/recall/F1 per class; annotate thresholds when discussing trade‑offs.
"t#G+=|yj|&"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you interpret logistic‑regression coefficients after standardization?	A positive standardized coefficient means increasing the feature increases log‑odds of the positive class; magnitude indicates relative influence; convert to odds ratios with exp(coef) for interpretability.
c?/.`~29}G	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How should you set random seeds?	Specify random_state consistently in splitters and estimators that support it to improve reproducibility; still report confidence intervals or variance across CV folds because randomness remains.
w=1I=@2z/V	Basic	Launch into Computing::Unit 05 - Data Science and Storage	When is dimensionality reduction helpful in this pipeline?	When p is large relative to n, or features are collinear/noisy; PCA or truncated SVD (for sparse) can reduce dimensionality before distance‑based models; include the reducer inside the pipeline and tune components via CV.
q@ZRdvLZ+9	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is the role of threshold tuning in classification?	After choosing a model, adjust the decision threshold on validation data to meet utility constraints (e.g., maximize F1, constrain recall ≥ target); document the chosen threshold and rationale.
"jAEa8cW#0B"	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How can visualizations assist clustering interpretation?	Project data with PCA/UMAP to 2D for intuition, color by cluster, plot cluster centers or medoids, and inspect feature‑wise distributions per cluster to derive human‑interpretable profiles.
qcAF]7_lNp	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What practical caveat applies to dendrogram cut selection?	Choosing a cut height is subjective; prefer stability criteria (e.g., looking for large vertical gaps) and sanity‑check with external metrics or domain validation; Ward linkage often yields compact, interpretable clusters.
Pnah3vbM()	Basic	Launch into Computing::Unit 05 - Data Science and Storage	Why are pipelines essential when comparing multiple classifiers?	They ensure consistent preprocessing across models, reduce boilerplate, make CV and hyperparameter tuning safer, and produce a single artifact for deployment—minimizing accidental inconsistencies between models.
MfWyK$5]l3	Basic	Launch into Computing::Unit 05 - Data Science and Storage	How do you check whether preprocessing helped?	Compare CV scores with and without scaling/selection; inspect learning curves for bias/variance changes; review coefficient stability for LR or margin widths for SVM.
fmj0AT>&`e	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What simple baseline should always be recorded?	A “dummy” baseline (e.g., majority class or stratified random) via DummyClassifier; it contextualizes model lifts and prevents celebrating marginal improvements over trivial predictors.
G1BOW`R-0p	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What is a clean way to report final results from this pipeline?	Report the selected model and hyperparameters, CV estimate with variance, final test metrics (accuracy, precision, recall, F1, ROC‑AUC), confusion matrix, any calibrated threshold, and key plots—with notes on data splits and randomness control.
HxQG.[[$`e	Basic	Launch into Computing::Unit 05 - Data Science and Storage	What should you document for reproducibility and auditability?	Data source and version, preprocessing steps and encoders, feature lists and selectors, random seeds, library versions, CV protocol, hyperparameter grids, chosen metrics/thresholds, and the final serialized pipeline artifact.
